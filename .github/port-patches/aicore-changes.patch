diff --git a/src/aiCore/legacy/clients/aws/AwsBedrockAPIClient.ts b/src/aiCore/legacy/clients/aws/AwsBedrockAPIClient.ts
index 94f74ee..c4b0140 100644
--- a/src/aiCore/legacy/clients/aws/AwsBedrockAPIClient.ts
+++ b/src/aiCore/legacy/clients/aws/AwsBedrockAPIClient.ts
@@ -1,6 +1,7 @@
 import { BedrockClient, ListFoundationModelsCommand, ListInferenceProfilesCommand } from '@aws-sdk/client-bedrock'
 import {
   BedrockRuntimeClient,
+  type BedrockRuntimeClientConfig,
   ConverseCommand,
   InvokeModelCommand,
   InvokeModelWithResponseStreamCommand
@@ -11,6 +12,8 @@ import { DEFAULT_MAX_TOKENS } from '@renderer/config/constant'
 import { findTokenLimit, isReasoningModel } from '@renderer/config/models'
 import {
   getAwsBedrockAccessKeyId,
+  getAwsBedrockApiKey,
+  getAwsBedrockAuthType,
   getAwsBedrockRegion,
   getAwsBedrockSecretAccessKey
 } from '@renderer/hooks/useAwsBedrock'
@@ -75,32 +78,48 @@ export class AwsBedrockAPIClient extends BaseApiClient<
     }
 
     const region = getAwsBedrockRegion()
-    const accessKeyId = getAwsBedrockAccessKeyId()
-    const secretAccessKey = getAwsBedrockSecretAccessKey()
+    const authType = getAwsBedrockAuthType()
 
     if (!region) {
-      throw new Error('AWS region is required. Please configure AWS-Region in extra headers.')
+      throw new Error('AWS region is required. Please configure AWS region in settings.')
     }
 
-    if (!accessKeyId || !secretAccessKey) {
-      throw new Error('AWS credentials are required. Please configure AWS-Access-Key-ID and AWS-Secret-Access-Key.')
-    }
+    // Build client configuration based on auth type
+    let clientConfig: BedrockRuntimeClientConfig
+
+    if (authType === 'iam') {
+      // IAM credentials authentication
+      const accessKeyId = getAwsBedrockAccessKeyId()
+      const secretAccessKey = getAwsBedrockSecretAccessKey()
 
-    const client = new BedrockRuntimeClient({
-      region,
-      credentials: {
-        accessKeyId,
-        secretAccessKey
+      if (!accessKeyId || !secretAccessKey) {
+        throw new Error('AWS credentials are required. Please configure Access Key ID and Secret Access Key.')
+      }
+
+      clientConfig = {
+        region,
+        credentials: {
+          accessKeyId,
+          secretAccessKey
+        }
       }
-    })
+    } else {
+      // API Key authentication
+      const awsBedrockApiKey = getAwsBedrockApiKey()
 
-    const bedrockClient = new BedrockClient({
-      region,
-      credentials: {
-        accessKeyId,
-        secretAccessKey
+      if (!awsBedrockApiKey) {
+        throw new Error('AWS Bedrock API Key is required. Please configure API Key in settings.')
       }
-    })
+
+      clientConfig = {
+        region,
+        token: { token: awsBedrockApiKey },
+        authSchemePreference: ['httpBearerAuth']
+      }
+    }
+
+    const client = new BedrockRuntimeClient(clientConfig)
+    const bedrockClient = new BedrockClient(clientConfig)
 
     this.sdkInstance = { client, bedrockClient, region }
     return this.sdkInstance
diff --git a/src/aiCore/legacy/clients/openai/OpenAIApiClient.ts b/src/aiCore/legacy/clients/openai/OpenAIApiClient.ts
index 239890c..8ff25e3 100644
--- a/src/aiCore/legacy/clients/openai/OpenAIApiClient.ts
+++ b/src/aiCore/legacy/clients/openai/OpenAIApiClient.ts
@@ -192,7 +192,7 @@ export class OpenAIAPIClient extends OpenAIBaseClient<
             extra_body: {
               google: {
                 thinking_config: {
-                  thinkingBudget: 0
+                  thinking_budget: 0
                 }
               }
             }
@@ -327,8 +327,8 @@ export class OpenAIAPIClient extends OpenAIBaseClient<
           extra_body: {
             google: {
               thinking_config: {
-                thinkingBudget: -1,
-                includeThoughts: true
+                thinking_budget: -1,
+                include_thoughts: true
               }
             }
           }
@@ -338,8 +338,8 @@ export class OpenAIAPIClient extends OpenAIBaseClient<
         extra_body: {
           google: {
             thinking_config: {
-              thinkingBudget: budgetTokens,
-              includeThoughts: true
+              thinking_budget: budgetTokens,
+              include_thoughts: true
             }
           }
         }
@@ -670,7 +670,7 @@ export class OpenAIAPIClient extends OpenAIBaseClient<
             } else if (isClaudeReasoningModel(model) && reasoningEffort.thinking?.budget_tokens) {
               suffix = ` --thinking_budget ${reasoningEffort.thinking.budget_tokens}`
             } else if (isGeminiReasoningModel(model) && reasoningEffort.extra_body?.google?.thinking_config) {
-              suffix = ` --thinking_budget ${reasoningEffort.extra_body.google.thinking_config.thinkingBudget}`
+              suffix = ` --thinking_budget ${reasoningEffort.extra_body.google.thinking_config.thinking_budget}`
             }
             // FIXME: poe 不支持多个text part，上传文本文件的时候用的不是file part而是text part，因此会出问题
             // 临时解决方案是强制poe用string content，但是其实poe部分支持array
diff --git a/src/aiCore/prepareParams/modelCapabilities.ts b/src/aiCore/prepareParams/modelCapabilities.ts
index 4a3c3f4..b6e4b25 100644
--- a/src/aiCore/prepareParams/modelCapabilities.ts
+++ b/src/aiCore/prepareParams/modelCapabilities.ts
@@ -85,6 +85,19 @@ export function supportsLargeFileUpload(model: Model): boolean {
   })
 }
 
+/**
+ * 检查模型是否支持TopP
+ */
+export function supportsTopP(model: Model): boolean {
+  const provider = getProviderByModel(model)
+
+  if (provider?.type === 'anthropic' || model?.endpoint_type === 'anthropic') {
+    return false
+  }
+
+  return true
+}
+
 /**
  * 获取提供商特定的文件大小限制
  */
diff --git a/src/aiCore/prepareParams/parameterBuilder.ts b/src/aiCore/prepareParams/parameterBuilder.ts
index d3fa1cb..397c481 100644
--- a/src/aiCore/prepareParams/parameterBuilder.ts
+++ b/src/aiCore/prepareParams/parameterBuilder.ts
@@ -34,6 +34,7 @@ import { setupToolsConfig } from '../utils/mcp'
 import { buildProviderOptions } from '../utils/options'
 import { getAnthropicThinkingBudget } from '../utils/reasoning'
 import { buildProviderBuiltinWebSearchConfig } from '../utils/websearch'
+import { supportsTopP } from './modelCapabilities'
 import { getTemperature, getTopP } from './modelParameters'
 
 const logger = loggerService.withContext('parameterBuilder')
@@ -176,20 +177,27 @@ export async function buildStreamTextParams(
     messages: sdkMessages,
     maxOutputTokens: maxTokens,
     temperature: getTemperature(assistant, model),
-    topP: getTopP(assistant, model),
     abortSignal: options.requestOptions?.signal,
     headers: options.requestOptions?.headers,
     providerOptions,
     stopWhen: stepCountIs(20),
     maxRetries: 0
   }
+
+  if (supportsTopP(model)) {
+    params.topP = getTopP(assistant, model)
+  }
+
   if (tools) {
     params.tools = tools
   }
+
   if (assistant.prompt) {
     params.system = await replacePromptVariables(assistant.prompt, model.name)
   }
+
   logger.debug('params', params)
+
   return {
     params,
     modelId: model.id,
diff --git a/src/aiCore/provider/__tests__/providerConfig.test.ts b/src/aiCore/provider/__tests__/providerConfig.test.ts
index eb6e73c..3978623 100644
--- a/src/aiCore/provider/__tests__/providerConfig.test.ts
+++ b/src/aiCore/provider/__tests__/providerConfig.test.ts
@@ -21,10 +21,45 @@ vi.mock('@renderer/store', () => ({
   }
 }))
 
+vi.mock('@renderer/utils/api', () => ({
+  formatApiHost: vi.fn((host, isSupportedAPIVersion = true) => {
+    if (isSupportedAPIVersion === false) {
+      return host // Return host as-is when isSupportedAPIVersion is false
+    }
+    return `${host}/v1` // Default behavior when isSupportedAPIVersion is true
+  }),
+  routeToEndpoint: vi.fn((host) => ({
+    baseURL: host,
+    endpoint: '/chat/completions'
+  }))
+}))
+
+vi.mock('@renderer/config/providers', async (importOriginal) => {
+  const actual = (await importOriginal()) as any
+  return {
+    ...actual,
+    isCherryAIProvider: vi.fn(),
+    isPerplexityProvider: vi.fn(),
+    isAnthropicProvider: vi.fn(() => false),
+    isAzureOpenAIProvider: vi.fn(() => false),
+    isGeminiProvider: vi.fn(() => false),
+    isNewApiProvider: vi.fn(() => false)
+  }
+})
+
+vi.mock('@renderer/hooks/useVertexAI', () => ({
+  isVertexProvider: vi.fn(() => false),
+  isVertexAIConfigured: vi.fn(() => false),
+  createVertexProvider: vi.fn()
+}))
+
+import { isCherryAIProvider, isPerplexityProvider } from '@renderer/config/providers'
+import { getProviderByModel } from '@renderer/services/AssistantService'
 import type { Model, Provider } from '@renderer/types'
+import { formatApiHost } from '@renderer/utils/api'
 
 import { COPILOT_DEFAULT_HEADERS, COPILOT_EDITOR_VERSION, isCopilotResponsesModel } from '../constants'
-import { providerToAiSdkConfig } from '../providerConfig'
+import { getActualProvider, providerToAiSdkConfig } from '../providerConfig'
 
 const createWindowKeyv = () => {
   const store = new Map<string, string>()
@@ -46,11 +81,31 @@ const createCopilotProvider = (): Provider => ({
   isSystem: true
 })
 
-const createModel = (id: string, name = id): Model => ({
+const createModel = (id: string, name = id, provider = 'copilot'): Model => ({
   id,
   name,
-  provider: 'copilot',
-  group: 'copilot'
+  provider,
+  group: provider
+})
+
+const createCherryAIProvider = (): Provider => ({
+  id: 'cherryai',
+  type: 'openai',
+  name: 'CherryAI',
+  apiKey: 'test-key',
+  apiHost: 'https://api.cherryai.com',
+  models: [],
+  isSystem: false
+})
+
+const createPerplexityProvider = (): Provider => ({
+  id: 'perplexity',
+  type: 'openai',
+  name: 'Perplexity',
+  apiKey: 'test-key',
+  apiHost: 'https://api.perplexity.ai',
+  models: [],
+  isSystem: false
 })
 
 describe('Copilot responses routing', () => {
@@ -87,3 +142,134 @@ describe('Copilot responses routing', () => {
     expect(config.options.headers?.['Copilot-Integration-Id']).toBe(COPILOT_DEFAULT_HEADERS['Copilot-Integration-Id'])
   })
 })
+
+describe('CherryAI provider configuration', () => {
+  beforeEach(() => {
+    ;(globalThis as any).window = {
+      ...(globalThis as any).window,
+      keyv: createWindowKeyv()
+    }
+    vi.clearAllMocks()
+  })
+
+  it('formats CherryAI provider apiHost with false parameter', () => {
+    const provider = createCherryAIProvider()
+    const model = createModel('gpt-4', 'GPT-4', 'cherryai')
+
+    // Mock the functions to simulate CherryAI provider detection
+    vi.mocked(isCherryAIProvider).mockReturnValue(true)
+    vi.mocked(getProviderByModel).mockReturnValue(provider)
+
+    // Call getActualProvider which should trigger formatProviderApiHost
+    const actualProvider = getActualProvider(model)
+
+    // Verify that formatApiHost was called with false as the second parameter
+    expect(formatApiHost).toHaveBeenCalledWith('https://api.cherryai.com', false)
+    expect(actualProvider.apiHost).toBe('https://api.cherryai.com')
+  })
+
+  it('does not format non-CherryAI provider with false parameter', () => {
+    const provider = {
+      id: 'openai',
+      type: 'openai',
+      name: 'OpenAI',
+      apiKey: 'test-key',
+      apiHost: 'https://api.openai.com',
+      models: [],
+      isSystem: false
+    } as Provider
+    const model = createModel('gpt-4', 'GPT-4', 'openai')
+
+    // Mock the functions to simulate non-CherryAI provider
+    vi.mocked(isCherryAIProvider).mockReturnValue(false)
+    vi.mocked(getProviderByModel).mockReturnValue(provider)
+
+    // Call getActualProvider
+    const actualProvider = getActualProvider(model)
+
+    // Verify that formatApiHost was called with default parameters (true)
+    expect(formatApiHost).toHaveBeenCalledWith('https://api.openai.com')
+    expect(actualProvider.apiHost).toBe('https://api.openai.com/v1')
+  })
+
+  it('handles CherryAI provider with empty apiHost', () => {
+    const provider = createCherryAIProvider()
+    provider.apiHost = ''
+    const model = createModel('gpt-4', 'GPT-4', 'cherryai')
+
+    vi.mocked(isCherryAIProvider).mockReturnValue(true)
+    vi.mocked(getProviderByModel).mockReturnValue(provider)
+
+    const actualProvider = getActualProvider(model)
+
+    expect(formatApiHost).toHaveBeenCalledWith('', false)
+    expect(actualProvider.apiHost).toBe('')
+  })
+})
+
+describe('Perplexity provider configuration', () => {
+  beforeEach(() => {
+    ;(globalThis as any).window = {
+      ...(globalThis as any).window,
+      keyv: createWindowKeyv()
+    }
+    vi.clearAllMocks()
+  })
+
+  it('formats Perplexity provider apiHost with false parameter', () => {
+    const provider = createPerplexityProvider()
+    const model = createModel('sonar', 'Sonar', 'perplexity')
+
+    // Mock the functions to simulate Perplexity provider detection
+    vi.mocked(isCherryAIProvider).mockReturnValue(false)
+    vi.mocked(isPerplexityProvider).mockReturnValue(true)
+    vi.mocked(getProviderByModel).mockReturnValue(provider)
+
+    // Call getActualProvider which should trigger formatProviderApiHost
+    const actualProvider = getActualProvider(model)
+
+    // Verify that formatApiHost was called with false as the second parameter
+    expect(formatApiHost).toHaveBeenCalledWith('https://api.perplexity.ai', false)
+    expect(actualProvider.apiHost).toBe('https://api.perplexity.ai')
+  })
+
+  it('does not format non-Perplexity provider with false parameter', () => {
+    const provider = {
+      id: 'openai',
+      type: 'openai',
+      name: 'OpenAI',
+      apiKey: 'test-key',
+      apiHost: 'https://api.openai.com',
+      models: [],
+      isSystem: false
+    } as Provider
+    const model = createModel('gpt-4', 'GPT-4', 'openai')
+
+    // Mock the functions to simulate non-Perplexity provider
+    vi.mocked(isCherryAIProvider).mockReturnValue(false)
+    vi.mocked(isPerplexityProvider).mockReturnValue(false)
+    vi.mocked(getProviderByModel).mockReturnValue(provider)
+
+    // Call getActualProvider
+    const actualProvider = getActualProvider(model)
+
+    // Verify that formatApiHost was called with default parameters (true)
+    expect(formatApiHost).toHaveBeenCalledWith('https://api.openai.com')
+    expect(actualProvider.apiHost).toBe('https://api.openai.com/v1')
+  })
+
+  it('handles Perplexity provider with empty apiHost', () => {
+    const provider = createPerplexityProvider()
+    provider.apiHost = ''
+    const model = createModel('sonar', 'Sonar', 'perplexity')
+
+    vi.mocked(isCherryAIProvider).mockReturnValue(false)
+    vi.mocked(isPerplexityProvider).mockReturnValue(true)
+    vi.mocked(getProviderByModel).mockReturnValue(provider)
+
+    const actualProvider = getActualProvider(model)
+
+    expect(formatApiHost).toHaveBeenCalledWith('', false)
+    expect(actualProvider.apiHost).toBe('')
+  })
+})
diff --git a/src/aiCore/provider/config/aihubmix.ts b/src/aiCore/provider/config/aihubmix.ts
index 432be5e..8feed89 100644
--- a/src/aiCore/provider/config/aihubmix.ts
+++ b/src/aiCore/provider/config/aihubmix.ts
@@ -52,7 +52,7 @@ const AIHUBMIX_RULES: RuleSet = {
       }
     }
   ],
-  fallbackRule: (provider: Provider) => provider
+  fallbackRule: (provider: Provider) => extraProviderConfig(provider)
 }
 
 export const aihubmixProviderCreator = provider2Provider.bind(null, AIHUBMIX_RULES)
diff --git a/src/aiCore/provider/providerConfig.ts b/src/aiCore/provider/providerConfig.ts
index 1532375..7f279a3 100644
--- a/src/aiCore/provider/providerConfig.ts
+++ b/src/aiCore/provider/providerConfig.ts
@@ -9,11 +9,15 @@ import { isOpenAIChatCompletionOnlyModel } from '@renderer/config/models'
 import {
   isAnthropicProvider,
   isAzureOpenAIProvider,
+  isCherryAIProvider,
   isGeminiProvider,
-  isNewApiProvider
+  isNewApiProvider,
+  isPerplexityProvider
 } from '@renderer/config/providers'
 import {
   getAwsBedrockAccessKeyId,
+  getAwsBedrockApiKey,
+  getAwsBedrockAuthType,
   getAwsBedrockRegion,
   getAwsBedrockSecretAccessKey
 } from '@renderer/hooks/useAwsBedrock'
@@ -98,6 +102,10 @@ function formatProviderApiHost(provider: Provider): Provider {
     formatted.apiHost = formatAzureOpenAIApiHost(formatted.apiHost)
   } else if (isVertexProvider(formatted)) {
     formatted.apiHost = formatVertexApiHost(formatted)
+  } else if (isCherryAIProvider(formatted)) {
+    formatted.apiHost = formatApiHost(formatted.apiHost, false)
+  } else if (isPerplexityProvider(formatted)) {
+    formatted.apiHost = formatApiHost(formatted.apiHost, false)
   } else {
     formatted.apiHost = formatApiHost(formatted.apiHost)
   }
@@ -192,9 +200,15 @@ export function providerToAiSdkConfig(
 
   // bedrock
   if (aiSdkProviderId === 'bedrock') {
+    const authType = getAwsBedrockAuthType()
     extraOptions.region = getAwsBedrockRegion()
-    extraOptions.accessKeyId = getAwsBedrockAccessKeyId()
-    extraOptions.secretAccessKey = getAwsBedrockSecretAccessKey()
+
+    if (authType === 'apiKey') {
+      extraOptions.apiKey = getAwsBedrockApiKey()
+    } else {
+      extraOptions.accessKeyId = getAwsBedrockAccessKeyId()
+      extraOptions.secretAccessKey = getAwsBedrockSecretAccessKey()
+    }
   }
   // google-vertex
   if (aiSdkProviderId === 'google-vertex' || aiSdkProviderId === 'google-vertex-anthropic') {
diff --git a/src/aiCore/utils/options.ts b/src/aiCore/utils/options.ts
index eaf4764..60d9b1e 100644
--- a/src/aiCore/utils/options.ts
+++ b/src/aiCore/utils/options.ts
@@ -17,6 +17,7 @@ import { getAiSdkProviderId } from '../provider/factory'
 import { buildGeminiGenerateImageParams } from './image'
 import {
   getAnthropicReasoningParams,
+  getBedrockReasoningParams,
   getCustomParameters,
   getGeminiReasoningParams,
   getOpenAIReasoningParams,
@@ -127,6 +128,9 @@ export function buildProviderOptions(
         case 'google-vertex-anthropic':
           providerSpecificOptions = buildAnthropicProviderOptions(assistant, model, capabilities)
           break
+        case 'bedrock':
+          providerSpecificOptions = buildBedrockProviderOptions(assistant, model, capabilities)
+          break
         default:
           // 对于其他 provider，使用通用的构建逻辑
           providerSpecificOptions = {
@@ -266,6 +270,32 @@ function buildXAIProviderOptions(
   return providerOptions
 }
 
+/**
+ * Build Bedrock providerOptions
+ */
+function buildBedrockProviderOptions(
+  assistant: Assistant,
+  model: Model,
+  capabilities: {
+    enableReasoning: boolean
+    enableWebSearch: boolean
+    enableGenerateImage: boolean
+  }
+): Record<string, any> {
+  const { enableReasoning } = capabilities
+  let providerOptions: Record<string, any> = {}
+
+  if (enableReasoning) {
+    const reasoningParams = getBedrockReasoningParams(assistant, model)
+    providerOptions = {
+      ...providerOptions,
+      ...reasoningParams
+    }
+  }
+
+  return providerOptions
+}
+
 /**
  * 构建通用的 providerOptions（用于其他 provider）
  */
diff --git a/src/aiCore/utils/reasoning.ts b/src/aiCore/utils/reasoning.ts
index 0246ac3..1d7123a 100644
--- a/src/aiCore/utils/reasoning.ts
+++ b/src/aiCore/utils/reasoning.ts
@@ -98,7 +98,7 @@ export function getReasoningEffort(assistant: Assistant, model: Model): Reasonin
           extra_body: {
             google: {
               thinking_config: {
-                thinkingBudget: 0
+                thinking_budget: 0
               }
             }
           }
@@ -259,8 +259,8 @@ export function getReasoningEffort(assistant: Assistant, model: Model): Reasonin
         extra_body: {
           google: {
             thinking_config: {
-              thinkingBudget: -1,
-              includeThoughts: true
+              thinking_budget: -1,
+              include_thoughts: true
             }
           }
         }
@@ -270,8 +270,8 @@ export function getReasoningEffort(assistant: Assistant, model: Model): Reasonin
       extra_body: {
         google: {
           thinking_config: {
-            thinkingBudget: budgetTokens,
-            includeThoughts: true
+            thinking_budget: budgetTokens ?? -1,
+            include_thoughts: true
           }
         }
       }
@@ -418,6 +418,8 @@ export function getAnthropicReasoningParams(assistant: Assistant, model: Model):
 /**
  * 获取 Gemini 推理参数
  * 从 GeminiAPIClient 中提取的逻辑
+ * 注意：Gemini/GCP 端点所使用的 thinkingBudget 等参数应该按照驼峰命名法传递
+ * 而在 Google 官方提供的 OpenAI 兼容端点中则使用蛇形命名法 thinking_budget
  */
 export function getGeminiReasoningParams(assistant: Assistant, model: Model): Record<string, any> {
   if (!isReasoningModel(model)) {
@@ -485,6 +487,34 @@ export function getXAIReasoningParams(assistant: Assistant, model: Model): Recor
   }
 }
 
+/**
+ * Get Bedrock reasoning parameters
+ */
+export function getBedrockReasoningParams(assistant: Assistant, model: Model): Record<string, any> {
+  if (!isReasoningModel(model)) {
+    return {}
+  }
+
+  const reasoningEffort = assistant?.settings?.reasoning_effort
+
+  if (reasoningEffort === undefined) {
+    return {}
+  }
+
+  // Only apply thinking budget for Claude reasoning models
+  if (!isSupportedThinkingTokenClaudeModel(model)) {
+    return {}
+  }
+
+  const budgetTokens = getAnthropicThinkingBudget(assistant, model)
+  return {
+    reasoningConfig: {
+      type: 'enabled',
+      budgetTokens: budgetTokens
+    }
+  }
+}
+
 /**
  * 获取自定义参数
  * 从 assistant 设置中提取自定义参数
