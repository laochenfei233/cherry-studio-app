diff --git a/src/aiCore/chunk/AiSdkToChunkAdapter.ts b/src/aiCore/chunk/AiSdkToChunkAdapter.ts
index 3f27f94..6e4288d 100644
--- a/src/aiCore/chunk/AiSdkToChunkAdapter.ts
+++ b/src/aiCore/chunk/AiSdkToChunkAdapter.ts
@@ -4,8 +4,10 @@
  */
 
 import { loggerService } from '@logger'
-import { AISDKWebSearchResult, MCPTool, WebSearchResults, WebSearchSource } from '@renderer/types'
-import { Chunk, ChunkType } from '@renderer/types/chunk'
+import type { AISDKWebSearchResult, MCPTool, WebSearchResults } from '@renderer/types'
+import { WebSearchSource } from '@renderer/types'
+import type { Chunk } from '@renderer/types/chunk'
+import { ChunkType } from '@renderer/types/chunk'
 import { ProviderSpecificError } from '@renderer/types/provider-specific-error'
 import { formatErrorMessage } from '@renderer/utils/error'
 import { convertLinks, flushLinkConverterBuffer } from '@renderer/utils/linkConverter'
diff --git a/src/aiCore/chunk/handleToolCallChunk.ts b/src/aiCore/chunk/handleToolCallChunk.ts
index 4d18024..32c7e53 100644
--- a/src/aiCore/chunk/handleToolCallChunk.ts
+++ b/src/aiCore/chunk/handleToolCallChunk.ts
@@ -6,7 +6,7 @@
 
 import { loggerService } from '@logger'
 import { processKnowledgeReferences } from '@renderer/services/KnowledgeService'
-import {
+import type {
   BaseTool,
   MCPCallToolResponse,
   MCPTool,
@@ -14,7 +14,8 @@ import {
   MCPToolResultContent,
   NormalToolResponse
 } from '@renderer/types'
-import { Chunk, ChunkType } from '@renderer/types/chunk'
+import type { Chunk } from '@renderer/types/chunk'
+import { ChunkType } from '@renderer/types/chunk'
 import type { ToolSet, TypedToolCall, TypedToolError, TypedToolResult } from 'ai'
 
 const logger = loggerService.withContext('ToolCallChunkHandler')
diff --git a/src/aiCore/index_new.ts b/src/aiCore/index_new.ts
index f6d6673..800d2ff 100644
--- a/src/aiCore/index_new.ts
+++ b/src/aiCore/index_new.ts
@@ -11,7 +11,7 @@ import { createExecutor } from '@cherrystudio/ai-core'
 import { loggerService } from '@logger'
 import { getEnableDeveloperMode } from '@renderer/hooks/useSettings'
 import { addSpan, endSpan } from '@renderer/services/SpanManagerService'
-import { StartSpanParams } from '@renderer/trace/types/ModelSpanEntity'
+import type { StartSpanParams } from '@renderer/trace/types/ModelSpanEntity'
 import type { Assistant, GenerateImageParams, Model, Provider } from '@renderer/types'
 import type { AiSdkModel, StreamTextParams } from '@renderer/types/aiCoreTypes'
 import { SUPPORTED_IMAGE_ENDPOINT_LIST } from '@renderer/utils'
@@ -20,8 +20,9 @@ import { type ImageModel, type LanguageModel, type Provider as AiSdkProvider, wr
 
 import AiSdkToChunkAdapter from './chunk/AiSdkToChunkAdapter'
 import LegacyAiProvider from './legacy/index'
-import { CompletionsParams, CompletionsResult } from './legacy/middleware/schemas'
-import { AiSdkMiddlewareConfig, buildAiSdkMiddlewares } from './middleware/AiSdkMiddlewareBuilder'
+import type { CompletionsParams, CompletionsResult } from './legacy/middleware/schemas'
+import type { AiSdkMiddlewareConfig } from './middleware/AiSdkMiddlewareBuilder'
+import { buildAiSdkMiddlewares } from './middleware/AiSdkMiddlewareBuilder'
 import { buildPlugins } from './plugins/PluginBuilder'
 import { createAiSdkProvider } from './provider/factory'
 import {
diff --git a/src/aiCore/legacy/clients/ApiClientFactory.ts b/src/aiCore/legacy/clients/ApiClientFactory.ts
index e7194c2..bc41616 100644
--- a/src/aiCore/legacy/clients/ApiClientFactory.ts
+++ b/src/aiCore/legacy/clients/ApiClientFactory.ts
@@ -1,11 +1,11 @@
 import { loggerService } from '@logger'
 import { isNewApiProvider } from '@renderer/config/providers'
-import { Provider } from '@renderer/types'
+import type { Provider } from '@renderer/types'
 
 import { AihubmixAPIClient } from './aihubmix/AihubmixAPIClient'
 import { AnthropicAPIClient } from './anthropic/AnthropicAPIClient'
 import { AwsBedrockAPIClient } from './aws/AwsBedrockAPIClient'
-import { BaseApiClient } from './BaseApiClient'
+import type { BaseApiClient } from './BaseApiClient'
 import { CherryAiAPIClient } from './cherryai/CherryAiAPIClient'
 import { GeminiAPIClient } from './gemini/GeminiAPIClient'
 import { VertexAPIClient } from './gemini/VertexAPIClient'
diff --git a/src/aiCore/legacy/clients/BaseApiClient.ts b/src/aiCore/legacy/clients/BaseApiClient.ts
index 4a41db7..767cad1 100644
--- a/src/aiCore/legacy/clients/BaseApiClient.ts
+++ b/src/aiCore/legacy/clients/BaseApiClient.ts
@@ -9,29 +9,31 @@ import { REFERENCE_PROMPT } from '@renderer/config/prompts'
 import { isSupportServiceTierProvider } from '@renderer/config/providers'
 import { getLMStudioKeepAliveTime } from '@renderer/hooks/useLMStudio'
 import { getAssistantSettings } from '@renderer/services/AssistantService'
-import {
+import type {
   Assistant,
-  FileTypes,
   GenerateImageParams,
-  GroqServiceTiers,
-  isGroqServiceTier,
-  isOpenAIServiceTier,
   KnowledgeReference,
   MCPCallToolResponse,
   MCPTool,
   MCPToolResponse,
   MemoryItem,
   Model,
-  OpenAIServiceTiers,
   OpenAIVerbosity,
   Provider,
-  SystemProviderIds,
   ToolCallResponse,
   WebSearchProviderResponse,
   WebSearchResponse
 } from '@renderer/types'
-import { Message } from '@renderer/types/newMessage'
 import {
+  FileTypes,
+  GroqServiceTiers,
+  isGroqServiceTier,
+  isOpenAIServiceTier,
+  OpenAIServiceTiers,
+  SystemProviderIds
+} from '@renderer/types'
+import type { Message } from '@renderer/types/newMessage'
+import type {
   RequestOptions,
   SdkInstance,
   SdkMessageParam,
@@ -49,8 +51,8 @@ import { defaultTimeout } from '@shared/config/constant'
 import { defaultAppHeaders } from '@shared/utils'
 import { isEmpty } from 'lodash'
 
-import { CompletionsContext } from '../middleware/types'
-import { ApiClient, RequestTransformer, ResponseChunkTransformer } from './types'
+import type { CompletionsContext } from '../middleware/types'
+import type { ApiClient, RequestTransformer, ResponseChunkTransformer } from './types'
 
 const logger = loggerService.withContext('BaseApiClient')
 
diff --git a/src/aiCore/legacy/clients/MixedBaseApiClient.ts b/src/aiCore/legacy/clients/MixedBaseApiClient.ts
index 36a207e..fb5568a 100644
--- a/src/aiCore/legacy/clients/MixedBaseApiClient.ts
+++ b/src/aiCore/legacy/clients/MixedBaseApiClient.ts
@@ -1,4 +1,4 @@
-import {
+import type {
   GenerateImageParams,
   MCPCallToolResponse,
   MCPTool,
@@ -7,7 +7,7 @@ import {
   Provider,
   ToolCallResponse
 } from '@renderer/types'
-import {
+import type {
   RequestOptions,
   SdkInstance,
   SdkMessageParam,
@@ -19,13 +19,13 @@ import {
   SdkToolCall
 } from '@renderer/types/sdk'
 
-import { CompletionsContext } from '../middleware/types'
-import { AnthropicAPIClient } from './anthropic/AnthropicAPIClient'
+import type { CompletionsContext } from '../middleware/types'
+import type { AnthropicAPIClient } from './anthropic/AnthropicAPIClient'
 import { BaseApiClient } from './BaseApiClient'
-import { GeminiAPIClient } from './gemini/GeminiAPIClient'
-import { OpenAIAPIClient } from './openai/OpenAIApiClient'
-import { OpenAIResponseAPIClient } from './openai/OpenAIResponseAPIClient'
-import { RequestTransformer, ResponseChunkTransformer } from './types'
+import type { GeminiAPIClient } from './gemini/GeminiAPIClient'
+import type { OpenAIAPIClient } from './openai/OpenAIApiClient'
+import type { OpenAIResponseAPIClient } from './openai/OpenAIResponseAPIClient'
+import type { RequestTransformer, ResponseChunkTransformer } from './types'
 
 /**
  * MixedAPIClient - 适用于可能含有多种接口类型的Provider
diff --git a/src/aiCore/legacy/clients/__tests__/ApiClientFactory.test.ts b/src/aiCore/legacy/clients/__tests__/ApiClientFactory.test.ts
index b8870fd..03ec1e1 100644
--- a/src/aiCore/legacy/clients/__tests__/ApiClientFactory.test.ts
+++ b/src/aiCore/legacy/clients/__tests__/ApiClientFactory.test.ts
@@ -1,4 +1,4 @@
-import { Provider } from '@renderer/types'
+import type { Provider } from '@renderer/types'
 import { beforeEach, describe, expect, it, vi } from 'vitest'
 
 import { AihubmixAPIClient } from '../aihubmix/AihubmixAPIClient'
diff --git a/src/aiCore/legacy/clients/__tests__/index.clientCompatibilityTypes.test.ts b/src/aiCore/legacy/clients/__tests__/index.clientCompatibilityTypes.test.ts
index dd85730..bcff572 100644
--- a/src/aiCore/legacy/clients/__tests__/index.clientCompatibilityTypes.test.ts
+++ b/src/aiCore/legacy/clients/__tests__/index.clientCompatibilityTypes.test.ts
@@ -6,7 +6,7 @@ import { VertexAPIClient } from '@renderer/aiCore/legacy/clients/gemini/VertexAP
 import { NewAPIClient } from '@renderer/aiCore/legacy/clients/newapi/NewAPIClient'
 import { OpenAIAPIClient } from '@renderer/aiCore/legacy/clients/openai/OpenAIApiClient'
 import { OpenAIResponseAPIClient } from '@renderer/aiCore/legacy/clients/openai/OpenAIResponseAPIClient'
-import { EndpointType, Model, Provider } from '@renderer/types'
+import type { EndpointType, Model, Provider } from '@renderer/types'
 import { beforeEach, describe, expect, it, vi } from 'vitest'
 
 vi.mock('@renderer/config/models', () => ({
diff --git a/src/aiCore/legacy/clients/aihubmix/AihubmixAPIClient.ts b/src/aiCore/legacy/clients/aihubmix/AihubmixAPIClient.ts
index 1149c04..a8a0ca5 100644
--- a/src/aiCore/legacy/clients/aihubmix/AihubmixAPIClient.ts
+++ b/src/aiCore/legacy/clients/aihubmix/AihubmixAPIClient.ts
@@ -1,8 +1,8 @@
 import { isOpenAILLMModel } from '@renderer/config/models'
-import { Model, Provider } from '@renderer/types'
+import type { Model, Provider } from '@renderer/types'
 
 import { AnthropicAPIClient } from '../anthropic/AnthropicAPIClient'
-import { BaseApiClient } from '../BaseApiClient'
+import type { BaseApiClient } from '../BaseApiClient'
 import { GeminiAPIClient } from '../gemini/GeminiAPIClient'
 import { MixedBaseAPIClient } from '../MixedBaseApiClient'
 import { OpenAIAPIClient } from '../openai/OpenAIApiClient'
diff --git a/src/aiCore/legacy/clients/anthropic/AnthropicAPIClient.ts b/src/aiCore/legacy/clients/anthropic/AnthropicAPIClient.ts
index 4f9bb28..15f3cf1 100644
--- a/src/aiCore/legacy/clients/anthropic/AnthropicAPIClient.ts
+++ b/src/aiCore/legacy/clients/anthropic/AnthropicAPIClient.ts
@@ -1,5 +1,5 @@
-import Anthropic from '@anthropic-ai/sdk'
-import {
+import type Anthropic from '@anthropic-ai/sdk'
+import type {
   Base64ImageSource,
   ImageBlockParam,
   MessageParam,
@@ -8,7 +8,7 @@ import {
   ToolUseBlock,
   WebSearchTool20250305
 } from '@anthropic-ai/sdk/resources'
-import {
+import type {
   ContentBlock,
   ContentBlockParam,
   MessageCreateParamsBase,
@@ -23,27 +23,24 @@ import {
   WebSearchToolResultError
 } from '@anthropic-ai/sdk/resources/messages'
 import { MessageStream } from '@anthropic-ai/sdk/resources/messages/messages'
-import AnthropicVertex from '@anthropic-ai/vertex-sdk'
+import type AnthropicVertex from '@anthropic-ai/vertex-sdk'
 import { loggerService } from '@logger'
 import { DEFAULT_MAX_TOKENS } from '@renderer/config/constant'
 import { findTokenLimit, isClaudeReasoningModel, isReasoningModel, isWebSearchModel } from '@renderer/config/models'
 import { getAssistantSettings } from '@renderer/services/AssistantService'
 import FileManager from '@renderer/services/FileManager'
 import { estimateTextTokens } from '@renderer/services/TokenService'
-import {
+import type {
   Assistant,
-  EFFORT_RATIO,
-  FileTypes,
   MCPCallToolResponse,
   MCPTool,
   MCPToolResponse,
   Model,
   Provider,
-  ToolCallResponse,
-  WebSearchSource
+  ToolCallResponse
 } from '@renderer/types'
-import {
-  ChunkType,
+import { EFFORT_RATIO, FileTypes, WebSearchSource } from '@renderer/types'
+import type {
   ErrorChunk,
   LLMWebSearchCompleteChunk,
   LLMWebSearchInProgressChunk,
@@ -53,8 +50,9 @@ import {
   ThinkingDeltaChunk,
   ThinkingStartChunk
 } from '@renderer/types/chunk'
+import { ChunkType } from '@renderer/types/chunk'
 import { type Message } from '@renderer/types/newMessage'
-import {
+import type {
   AnthropicSdkMessageParam,
   AnthropicSdkParams,
   AnthropicSdkRawChunk,
@@ -71,9 +69,9 @@ import { findFileBlocks, findImageBlocks } from '@renderer/utils/messageUtils/fi
 import { buildClaudeCodeSystemMessage, getSdkClient } from '@shared/anthropic'
 import { t } from 'i18next'
 
-import { GenericChunk } from '../../middleware/schemas'
+import type { GenericChunk } from '../../middleware/schemas'
 import { BaseApiClient } from '../BaseApiClient'
-import { AnthropicStreamListener, RawStreamListener, RequestTransformer, ResponseChunkTransformer } from '../types'
+import type { AnthropicStreamListener, RawStreamListener, RequestTransformer, ResponseChunkTransformer } from '../types'
 
 const logger = loggerService.withContext('AnthropicAPIClient')
 
diff --git a/src/aiCore/legacy/clients/anthropic/AnthropicVertexClient.ts b/src/aiCore/legacy/clients/anthropic/AnthropicVertexClient.ts
index bb96ac9..2fe16e8 100644
--- a/src/aiCore/legacy/clients/anthropic/AnthropicVertexClient.ts
+++ b/src/aiCore/legacy/clients/anthropic/AnthropicVertexClient.ts
@@ -1,8 +1,8 @@
-import Anthropic from '@anthropic-ai/sdk'
+import type Anthropic from '@anthropic-ai/sdk'
 import AnthropicVertex from '@anthropic-ai/vertex-sdk'
 import { loggerService } from '@logger'
 import { getVertexAILocation, getVertexAIProjectId, getVertexAIServiceAccount } from '@renderer/hooks/useVertexAI'
-import { Provider } from '@renderer/types'
+import type { Provider } from '@renderer/types'
 import { isEmpty } from 'lodash'
 
 import { AnthropicAPIClient } from './AnthropicAPIClient'
diff --git a/src/aiCore/legacy/clients/aws/AwsBedrockAPIClient.ts b/src/aiCore/legacy/clients/aws/AwsBedrockAPIClient.ts
index 1de8a72..c4b0140 100644
--- a/src/aiCore/legacy/clients/aws/AwsBedrockAPIClient.ts
+++ b/src/aiCore/legacy/clients/aws/AwsBedrockAPIClient.ts
@@ -1,25 +1,26 @@
 import { BedrockClient, ListFoundationModelsCommand, ListInferenceProfilesCommand } from '@aws-sdk/client-bedrock'
 import {
   BedrockRuntimeClient,
+  type BedrockRuntimeClientConfig,
   ConverseCommand,
   InvokeModelCommand,
   InvokeModelWithResponseStreamCommand
 } from '@aws-sdk/client-bedrock-runtime'
 import { loggerService } from '@logger'
-import { GenericChunk } from '@renderer/aiCore/legacy/middleware/schemas'
+import type { GenericChunk } from '@renderer/aiCore/legacy/middleware/schemas'
 import { DEFAULT_MAX_TOKENS } from '@renderer/config/constant'
 import { findTokenLimit, isReasoningModel } from '@renderer/config/models'
 import {
   getAwsBedrockAccessKeyId,
+  getAwsBedrockApiKey,
+  getAwsBedrockAuthType,
   getAwsBedrockRegion,
   getAwsBedrockSecretAccessKey
 } from '@renderer/hooks/useAwsBedrock'
 import { getAssistantSettings } from '@renderer/services/AssistantService'
 import { estimateTextTokens } from '@renderer/services/TokenService'
-import {
+import type {
   Assistant,
-  EFFORT_RATIO,
-  FileTypes,
   GenerateImageParams,
   MCPCallToolResponse,
   MCPTool,
@@ -28,15 +29,11 @@ import {
   Provider,
   ToolCallResponse
 } from '@renderer/types'
-import {
-  ChunkType,
-  MCPToolCreatedChunk,
-  TextDeltaChunk,
-  ThinkingDeltaChunk,
-  ThinkingStartChunk
-} from '@renderer/types/chunk'
-import { Message } from '@renderer/types/newMessage'
-import {
+import { EFFORT_RATIO, FileTypes } from '@renderer/types'
+import type { MCPToolCreatedChunk, TextDeltaChunk, ThinkingDeltaChunk, ThinkingStartChunk } from '@renderer/types/chunk'
+import { ChunkType } from '@renderer/types/chunk'
+import type { Message } from '@renderer/types/newMessage'
+import type {
   AwsBedrockSdkInstance,
   AwsBedrockSdkMessageParam,
   AwsBedrockSdkParams,
@@ -58,7 +55,7 @@ import { findFileBlocks, findImageBlocks } from '@renderer/utils/messageUtils/fi
 import { t } from 'i18next'
 
 import { BaseApiClient } from '../BaseApiClient'
-import { RequestTransformer, ResponseChunkTransformer } from '../types'
+import type { RequestTransformer, ResponseChunkTransformer } from '../types'
 
 const logger = loggerService.withContext('AwsBedrockAPIClient')
 
@@ -81,32 +78,48 @@ export class AwsBedrockAPIClient extends BaseApiClient<
     }
 
     const region = getAwsBedrockRegion()
-    const accessKeyId = getAwsBedrockAccessKeyId()
-    const secretAccessKey = getAwsBedrockSecretAccessKey()
+    const authType = getAwsBedrockAuthType()
 
     if (!region) {
-      throw new Error('AWS region is required. Please configure AWS-Region in extra headers.')
+      throw new Error('AWS region is required. Please configure AWS region in settings.')
     }
 
-    if (!accessKeyId || !secretAccessKey) {
-      throw new Error('AWS credentials are required. Please configure AWS-Access-Key-ID and AWS-Secret-Access-Key.')
-    }
+    // Build client configuration based on auth type
+    let clientConfig: BedrockRuntimeClientConfig
+
+    if (authType === 'iam') {
+      // IAM credentials authentication
+      const accessKeyId = getAwsBedrockAccessKeyId()
+      const secretAccessKey = getAwsBedrockSecretAccessKey()
+
+      if (!accessKeyId || !secretAccessKey) {
+        throw new Error('AWS credentials are required. Please configure Access Key ID and Secret Access Key.')
+      }
+
+      clientConfig = {
+        region,
+        credentials: {
+          accessKeyId,
+          secretAccessKey
+        }
+      }
+    } else {
+      // API Key authentication
+      const awsBedrockApiKey = getAwsBedrockApiKey()
 
-    const client = new BedrockRuntimeClient({
-      region,
-      credentials: {
-        accessKeyId,
-        secretAccessKey
+      if (!awsBedrockApiKey) {
+        throw new Error('AWS Bedrock API Key is required. Please configure API Key in settings.')
       }
-    })
 
-    const bedrockClient = new BedrockClient({
-      region,
-      credentials: {
-        accessKeyId,
-        secretAccessKey
+      clientConfig = {
+        region,
+        token: { token: awsBedrockApiKey },
+        authSchemePreference: ['httpBearerAuth']
       }
-    })
+    }
+
+    const client = new BedrockRuntimeClient(clientConfig)
+    const bedrockClient = new BedrockClient(clientConfig)
 
     this.sdkInstance = { client, bedrockClient, region }
     return this.sdkInstance
diff --git a/src/aiCore/legacy/clients/cherryai/CherryAiAPIClient.ts b/src/aiCore/legacy/clients/cherryai/CherryAiAPIClient.ts
index 08e4d9d..b72e0a8 100644
--- a/src/aiCore/legacy/clients/cherryai/CherryAiAPIClient.ts
+++ b/src/aiCore/legacy/clients/cherryai/CherryAiAPIClient.ts
@@ -1,6 +1,6 @@
-import OpenAI from '@cherrystudio/openai'
-import { Provider } from '@renderer/types'
-import { OpenAISdkParams, OpenAISdkRawOutput } from '@renderer/types/sdk'
+import type OpenAI from '@cherrystudio/openai'
+import type { Provider } from '@renderer/types'
+import type { OpenAISdkParams, OpenAISdkRawOutput } from '@renderer/types/sdk'
 
 import { OpenAIAPIClient } from '../openai/OpenAIApiClient'
 
diff --git a/src/aiCore/legacy/clients/gemini/GeminiAPIClient.ts b/src/aiCore/legacy/clients/gemini/GeminiAPIClient.ts
index 33d34c7..27e659c 100644
--- a/src/aiCore/legacy/clients/gemini/GeminiAPIClient.ts
+++ b/src/aiCore/legacy/clients/gemini/GeminiAPIClient.ts
@@ -1,14 +1,9 @@
-import {
+import type {
   Content,
-  createPartFromUri,
   File,
   FunctionCall,
   GenerateContentConfig,
   GenerateImagesConfig,
-  GoogleGenAI,
-  HarmBlockThreshold,
-  HarmCategory,
-  Modality,
   Model as GeminiModel,
   Part,
   SafetySetting,
@@ -16,6 +11,7 @@ import {
   ThinkingConfig,
   Tool
 } from '@google/genai'
+import { createPartFromUri, GoogleGenAI, HarmBlockThreshold, HarmCategory, Modality } from '@google/genai'
 import { loggerService } from '@logger'
 import { nanoid } from '@reduxjs/toolkit'
 import {
@@ -26,11 +22,9 @@ import {
   isVisionModel
 } from '@renderer/config/models'
 import { estimateTextTokens } from '@renderer/services/TokenService'
-import {
+import type {
   Assistant,
-  EFFORT_RATIO,
   FileMetadata,
-  FileTypes,
   FileUploadResponse,
   GenerateImageParams,
   MCPCallToolResponse,
@@ -38,12 +32,13 @@ import {
   MCPToolResponse,
   Model,
   Provider,
-  ToolCallResponse,
-  WebSearchSource
+  ToolCallResponse
 } from '@renderer/types'
-import { ChunkType, LLMWebSearchCompleteChunk, TextStartChunk, ThinkingStartChunk } from '@renderer/types/chunk'
-import { Message } from '@renderer/types/newMessage'
-import {
+import { EFFORT_RATIO, FileTypes, WebSearchSource } from '@renderer/types'
+import type { LLMWebSearchCompleteChunk, TextStartChunk, ThinkingStartChunk } from '@renderer/types/chunk'
+import { ChunkType } from '@renderer/types/chunk'
+import type { Message } from '@renderer/types/newMessage'
+import type {
   GeminiOptions,
   GeminiSdkMessageParam,
   GeminiSdkParams,
@@ -62,9 +57,9 @@ import { findFileBlocks, findImageBlocks, getMainTextContent } from '@renderer/u
 import { defaultTimeout, MB } from '@shared/config/constant'
 import { t } from 'i18next'
 
-import { GenericChunk } from '../../middleware/schemas'
+import type { GenericChunk } from '../../middleware/schemas'
 import { BaseApiClient } from '../BaseApiClient'
-import { RequestTransformer, ResponseChunkTransformer } from '../types'
+import type { RequestTransformer, ResponseChunkTransformer } from '../types'
 
 const logger = loggerService.withContext('GeminiAPIClient')
 
diff --git a/src/aiCore/legacy/clients/gemini/VertexAPIClient.ts b/src/aiCore/legacy/clients/gemini/VertexAPIClient.ts
index 37e6677..49a96a8 100644
--- a/src/aiCore/legacy/clients/gemini/VertexAPIClient.ts
+++ b/src/aiCore/legacy/clients/gemini/VertexAPIClient.ts
@@ -1,7 +1,7 @@
 import { GoogleGenAI } from '@google/genai'
 import { loggerService } from '@logger'
 import { createVertexProvider, isVertexAIConfigured, isVertexProvider } from '@renderer/hooks/useVertexAI'
-import { Model, Provider, VertexProvider } from '@renderer/types'
+import type { Model, Provider, VertexProvider } from '@renderer/types'
 import { isEmpty } from 'lodash'
 
 import { AnthropicVertexClient } from '../anthropic/AnthropicVertexClient'
diff --git a/src/aiCore/legacy/clients/newapi/NewAPIClient.ts b/src/aiCore/legacy/clients/newapi/NewAPIClient.ts
index 58b349a..f3e04e0 100644
--- a/src/aiCore/legacy/clients/newapi/NewAPIClient.ts
+++ b/src/aiCore/legacy/clients/newapi/NewAPIClient.ts
@@ -1,10 +1,10 @@
 import { loggerService } from '@logger'
 import { isSupportedModel } from '@renderer/config/models'
-import { Model, Provider } from '@renderer/types'
-import { NewApiModel } from '@renderer/types/sdk'
+import type { Model, Provider } from '@renderer/types'
+import type { NewApiModel } from '@renderer/types/sdk'
 
 import { AnthropicAPIClient } from '../anthropic/AnthropicAPIClient'
-import { BaseApiClient } from '../BaseApiClient'
+import type { BaseApiClient } from '../BaseApiClient'
 import { GeminiAPIClient } from '../gemini/GeminiAPIClient'
 import { MixedBaseAPIClient } from '../MixedBaseApiClient'
 import { OpenAIAPIClient } from '../openai/OpenAIApiClient'
diff --git a/src/aiCore/legacy/clients/openai/OpenAIApiClient.ts b/src/aiCore/legacy/clients/openai/OpenAIApiClient.ts
index 618d9b4..8ff25e3 100644
--- a/src/aiCore/legacy/clients/openai/OpenAIApiClient.ts
+++ b/src/aiCore/legacy/clients/openai/OpenAIApiClient.ts
@@ -1,5 +1,6 @@
-import OpenAI, { AzureOpenAI } from '@cherrystudio/openai'
-import {
+import type { AzureOpenAI } from '@cherrystudio/openai'
+import type OpenAI from '@cherrystudio/openai'
+import type {
   ChatCompletionContentPart,
   ChatCompletionContentPartRefusal,
   ChatCompletionTool
@@ -48,25 +49,28 @@ import { mapLanguageToQwenMTModel } from '@renderer/config/translate'
 import { processPostsuffixQwen3Model, processReqMessages } from '@renderer/services/ModelMessageService'
 import { estimateTextTokens } from '@renderer/services/TokenService'
 // For Copilot token
-import {
+import type {
   Assistant,
-  EFFORT_RATIO,
-  FileTypes,
-  isSystemProvider,
-  isTranslateAssistant,
   MCPCallToolResponse,
   MCPTool,
   MCPToolResponse,
   Model,
   OpenAIServiceTier,
   Provider,
+  ToolCallResponse
+} from '@renderer/types'
+import {
+  EFFORT_RATIO,
+  FileTypes,
+  isSystemProvider,
+  isTranslateAssistant,
   SystemProviderIds,
-  ToolCallResponse,
   WebSearchSource
 } from '@renderer/types'
-import { ChunkType, TextStartChunk, ThinkingStartChunk } from '@renderer/types/chunk'
-import { Message } from '@renderer/types/newMessage'
-import {
+import type { TextStartChunk, ThinkingStartChunk } from '@renderer/types/chunk'
+import { ChunkType } from '@renderer/types/chunk'
+import type { Message } from '@renderer/types/newMessage'
+import type {
   OpenAIExtraBody,
   OpenAIModality,
   OpenAISdkMessageParam,
@@ -86,8 +90,8 @@ import {
 import { findFileBlocks, findImageBlocks } from '@renderer/utils/messageUtils/find'
 import { t } from 'i18next'
 
-import { GenericChunk } from '../../middleware/schemas'
-import { RequestTransformer, ResponseChunkTransformer, ResponseChunkTransformerContext } from '../types'
+import type { GenericChunk } from '../../middleware/schemas'
+import type { RequestTransformer, ResponseChunkTransformer, ResponseChunkTransformerContext } from '../types'
 import { OpenAIBaseClient } from './OpenAIBaseClient'
 
 const logger = loggerService.withContext('OpenAIApiClient')
@@ -188,7 +192,7 @@ export class OpenAIAPIClient extends OpenAIBaseClient<
             extra_body: {
               google: {
                 thinking_config: {
-                  thinkingBudget: 0
+                  thinking_budget: 0
                 }
               }
             }
@@ -323,8 +327,8 @@ export class OpenAIAPIClient extends OpenAIBaseClient<
           extra_body: {
             google: {
               thinking_config: {
-                thinkingBudget: -1,
-                includeThoughts: true
+                thinking_budget: -1,
+                include_thoughts: true
               }
             }
           }
@@ -334,8 +338,8 @@ export class OpenAIAPIClient extends OpenAIBaseClient<
         extra_body: {
           google: {
             thinking_config: {
-              thinkingBudget: budgetTokens,
-              includeThoughts: true
+              thinking_budget: budgetTokens,
+              include_thoughts: true
             }
           }
         }
@@ -666,7 +670,7 @@ export class OpenAIAPIClient extends OpenAIBaseClient<
             } else if (isClaudeReasoningModel(model) && reasoningEffort.thinking?.budget_tokens) {
               suffix = ` --thinking_budget ${reasoningEffort.thinking.budget_tokens}`
             } else if (isGeminiReasoningModel(model) && reasoningEffort.extra_body?.google?.thinking_config) {
-              suffix = ` --thinking_budget ${reasoningEffort.extra_body.google.thinking_config.thinkingBudget}`
+              suffix = ` --thinking_budget ${reasoningEffort.extra_body.google.thinking_config.thinking_budget}`
             }
             // FIXME: poe 不支持多个text part，上传文本文件的时候用的不是file part而是text part，因此会出问题
             // 临时解决方案是强制poe用string content，但是其实poe部分支持array
diff --git a/src/aiCore/legacy/clients/openai/OpenAIBaseClient.ts b/src/aiCore/legacy/clients/openai/OpenAIBaseClient.ts
index 8a0a3fe..abd1793 100644
--- a/src/aiCore/legacy/clients/openai/OpenAIBaseClient.ts
+++ b/src/aiCore/legacy/clients/openai/OpenAIBaseClient.ts
@@ -10,9 +10,9 @@ import {
 import { getStoreSetting } from '@renderer/hooks/useSettings'
 import { getAssistantSettings } from '@renderer/services/AssistantService'
 import store from '@renderer/store'
-import { SettingsState } from '@renderer/store/settings'
-import { Assistant, GenerateImageParams, Model, Provider } from '@renderer/types'
-import {
+import type { SettingsState } from '@renderer/store/settings'
+import type { Assistant, GenerateImageParams, Model, Provider } from '@renderer/types'
+import type {
   OpenAIResponseSdkMessageParam,
   OpenAIResponseSdkParams,
   OpenAIResponseSdkRawChunk,
diff --git a/src/aiCore/legacy/clients/openai/OpenAIResponseAPIClient.ts b/src/aiCore/legacy/clients/openai/OpenAIResponseAPIClient.ts
index 5d13d6f..b9131be 100644
--- a/src/aiCore/legacy/clients/openai/OpenAIResponseAPIClient.ts
+++ b/src/aiCore/legacy/clients/openai/OpenAIResponseAPIClient.ts
@@ -1,8 +1,8 @@
 import OpenAI, { AzureOpenAI } from '@cherrystudio/openai'
-import { ResponseInput } from '@cherrystudio/openai/resources/responses/responses'
+import type { ResponseInput } from '@cherrystudio/openai/resources/responses/responses'
 import { loggerService } from '@logger'
-import { GenericChunk } from '@renderer/aiCore/legacy/middleware/schemas'
-import { CompletionsContext } from '@renderer/aiCore/legacy/middleware/types'
+import type { GenericChunk } from '@renderer/aiCore/legacy/middleware/schemas'
+import type { CompletionsContext } from '@renderer/aiCore/legacy/middleware/types'
 import {
   isGPT5SeriesModel,
   isOpenAIChatCompletionOnlyModel,
@@ -14,21 +14,20 @@ import {
 } from '@renderer/config/models'
 import { isSupportDeveloperRoleProvider } from '@renderer/config/providers'
 import { estimateTextTokens } from '@renderer/services/TokenService'
-import {
+import type {
   FileMetadata,
-  FileTypes,
   MCPCallToolResponse,
   MCPTool,
   MCPToolResponse,
   Model,
   OpenAIServiceTier,
   Provider,
-  ToolCallResponse,
-  WebSearchSource
+  ToolCallResponse
 } from '@renderer/types'
+import { FileTypes, WebSearchSource } from '@renderer/types'
 import { ChunkType } from '@renderer/types/chunk'
-import { Message } from '@renderer/types/newMessage'
-import {
+import type { Message } from '@renderer/types/newMessage'
+import type {
   OpenAIResponseSdkMessageParam,
   OpenAIResponseSdkParams,
   OpenAIResponseSdkRawChunk,
@@ -48,7 +47,7 @@ import { MB } from '@shared/config/constant'
 import { t } from 'i18next'
 import { isEmpty } from 'lodash'
 
-import { RequestTransformer, ResponseChunkTransformer } from '../types'
+import type { RequestTransformer, ResponseChunkTransformer } from '../types'
 import { OpenAIAPIClient } from './OpenAIApiClient'
 import { OpenAIBaseClient } from './OpenAIBaseClient'
 
diff --git a/src/aiCore/legacy/clients/ovms/OVMSClient.ts b/src/aiCore/legacy/clients/ovms/OVMSClient.ts
index 5dc9155..179bb54 100644
--- a/src/aiCore/legacy/clients/ovms/OVMSClient.ts
+++ b/src/aiCore/legacy/clients/ovms/OVMSClient.ts
@@ -1,7 +1,8 @@
-import OpenAI from '@cherrystudio/openai'
+import type OpenAI from '@cherrystudio/openai'
 import { loggerService } from '@logger'
 import { isSupportedModel } from '@renderer/config/models'
-import { objectKeys, Provider } from '@renderer/types'
+import type { Provider } from '@renderer/types'
+import { objectKeys } from '@renderer/types'
 
 import { OpenAIAPIClient } from '../openai/OpenAIApiClient'
 
diff --git a/src/aiCore/legacy/clients/ppio/PPIOAPIClient.ts b/src/aiCore/legacy/clients/ppio/PPIOAPIClient.ts
index 57b54b9..345496e 100644
--- a/src/aiCore/legacy/clients/ppio/PPIOAPIClient.ts
+++ b/src/aiCore/legacy/clients/ppio/PPIOAPIClient.ts
@@ -1,7 +1,7 @@
-import OpenAI from '@cherrystudio/openai'
+import type OpenAI from '@cherrystudio/openai'
 import { loggerService } from '@logger'
 import { isSupportedModel } from '@renderer/config/models'
-import { Model, Provider } from '@renderer/types'
+import type { Model, Provider } from '@renderer/types'
 
 import { OpenAIAPIClient } from '../openai/OpenAIApiClient'
 
diff --git a/src/aiCore/legacy/clients/types.ts b/src/aiCore/legacy/clients/types.ts
index 6d10f42..bf7b129 100644
--- a/src/aiCore/legacy/clients/types.ts
+++ b/src/aiCore/legacy/clients/types.ts
@@ -1,8 +1,8 @@
-import Anthropic from '@anthropic-ai/sdk'
-import OpenAI from '@cherrystudio/openai'
-import { Assistant, MCPTool, MCPToolResponse, Model, ToolCallResponse } from '@renderer/types'
-import { Provider } from '@renderer/types'
-import {
+import type Anthropic from '@anthropic-ai/sdk'
+import type OpenAI from '@cherrystudio/openai'
+import type { Assistant, MCPTool, MCPToolResponse, Model, ToolCallResponse } from '@renderer/types'
+import type { Provider } from '@renderer/types'
+import type {
   AnthropicSdkRawChunk,
   OpenAIResponseSdkRawChunk,
   OpenAIResponseSdkRawOutput,
@@ -15,8 +15,8 @@ import {
   SdkToolCall
 } from '@renderer/types/sdk'
 
-import { CompletionsParams, GenericChunk } from '../middleware/schemas'
-import { CompletionsContext } from '../middleware/types'
+import type { CompletionsParams, GenericChunk } from '../middleware/schemas'
+import type { CompletionsContext } from '../middleware/types'
 
 /**
  * 原始流监听器接口
diff --git a/src/aiCore/legacy/clients/zhipu/ZhipuAPIClient.ts b/src/aiCore/legacy/clients/zhipu/ZhipuAPIClient.ts
index c04e08f..ea6c141 100644
--- a/src/aiCore/legacy/clients/zhipu/ZhipuAPIClient.ts
+++ b/src/aiCore/legacy/clients/zhipu/ZhipuAPIClient.ts
@@ -1,7 +1,7 @@
-import OpenAI from '@cherrystudio/openai'
+import type OpenAI from '@cherrystudio/openai'
 import { loggerService } from '@logger'
-import { Provider } from '@renderer/types'
-import { GenerateImageParams } from '@renderer/types'
+import type { Provider } from '@renderer/types'
+import type { GenerateImageParams } from '@renderer/types'
 
 import { OpenAIAPIClient } from '../openai/OpenAIApiClient'
 
diff --git a/src/aiCore/legacy/index.ts b/src/aiCore/legacy/index.ts
index adc81f0..da6cdb6 100644
--- a/src/aiCore/legacy/index.ts
+++ b/src/aiCore/legacy/index.ts
@@ -1,10 +1,10 @@
 import { loggerService } from '@logger'
 import { ApiClientFactory } from '@renderer/aiCore/legacy/clients/ApiClientFactory'
-import { BaseApiClient } from '@renderer/aiCore/legacy/clients/BaseApiClient'
+import type { BaseApiClient } from '@renderer/aiCore/legacy/clients/BaseApiClient'
 import { isDedicatedImageGenerationModel, isFunctionCallingModel } from '@renderer/config/models'
 import { getProviderByModel } from '@renderer/services/AssistantService'
 import { withSpanResult } from '@renderer/services/SpanManagerService'
-import { StartSpanParams } from '@renderer/trace/types/ModelSpanEntity'
+import type { StartSpanParams } from '@renderer/trace/types/ModelSpanEntity'
 import type { GenerateImageParams, Model, Provider } from '@renderer/types'
 import type { RequestOptions, SdkModel } from '@renderer/types/sdk'
 import { isSupportedToolUse } from '@renderer/utils/mcp-tools'
diff --git a/src/aiCore/legacy/middleware/builder.ts b/src/aiCore/legacy/middleware/builder.ts
index 2ea20d4..1d0b9d1 100644
--- a/src/aiCore/legacy/middleware/builder.ts
+++ b/src/aiCore/legacy/middleware/builder.ts
@@ -1,7 +1,7 @@
 import { loggerService } from '@logger'
 
 import { DefaultCompletionsNamedMiddlewares } from './register'
-import { BaseContext, CompletionsMiddleware, MethodMiddleware } from './types'
+import type { BaseContext, CompletionsMiddleware, MethodMiddleware } from './types'
 
 const logger = loggerService.withContext('aiCore:MiddlewareBuilder')
 
diff --git a/src/aiCore/legacy/middleware/common/AbortHandlerMiddleware.ts b/src/aiCore/legacy/middleware/common/AbortHandlerMiddleware.ts
index a733e45..5f24797 100644
--- a/src/aiCore/legacy/middleware/common/AbortHandlerMiddleware.ts
+++ b/src/aiCore/legacy/middleware/common/AbortHandlerMiddleware.ts
@@ -1,8 +1,9 @@
 import { loggerService } from '@logger'
-import { Chunk, ChunkType, ErrorChunk } from '@renderer/types/chunk'
+import type { Chunk, ErrorChunk } from '@renderer/types/chunk'
+import { ChunkType } from '@renderer/types/chunk'
 import { addAbortController, removeAbortController } from '@renderer/utils/abortController'
 
-import { CompletionsParams, CompletionsResult } from '../schemas'
+import type { CompletionsParams, CompletionsResult } from '../schemas'
 import type { CompletionsContext, CompletionsMiddleware } from '../types'
 
 const logger = loggerService.withContext('aiCore:AbortHandlerMiddleware')
diff --git a/src/aiCore/legacy/middleware/common/ErrorHandlerMiddleware.ts b/src/aiCore/legacy/middleware/common/ErrorHandlerMiddleware.ts
index dde98cb..7d6a7f6 100644
--- a/src/aiCore/legacy/middleware/common/ErrorHandlerMiddleware.ts
+++ b/src/aiCore/legacy/middleware/common/ErrorHandlerMiddleware.ts
@@ -1,10 +1,10 @@
 import { loggerService } from '@logger'
 import { isZhipuModel } from '@renderer/config/models'
 import { getStoreProviders } from '@renderer/hooks/useStore'
-import { Chunk } from '@renderer/types/chunk'
+import type { Chunk } from '@renderer/types/chunk'
 
-import { CompletionsParams, CompletionsResult } from '../schemas'
-import { CompletionsContext } from '../types'
+import type { CompletionsParams, CompletionsResult } from '../schemas'
+import type { CompletionsContext } from '../types'
 import { createErrorChunk } from '../utils'
 
 const logger = loggerService.withContext('ErrorHandlerMiddleware')
diff --git a/src/aiCore/legacy/middleware/common/FinalChunkConsumerMiddleware.ts b/src/aiCore/legacy/middleware/common/FinalChunkConsumerMiddleware.ts
index 57498b9..0325e4e 100644
--- a/src/aiCore/legacy/middleware/common/FinalChunkConsumerMiddleware.ts
+++ b/src/aiCore/legacy/middleware/common/FinalChunkConsumerMiddleware.ts
@@ -1,10 +1,10 @@
 import { loggerService } from '@logger'
-import { Usage } from '@renderer/types'
+import type { Usage } from '@renderer/types'
 import type { Chunk } from '@renderer/types/chunk'
 import { ChunkType } from '@renderer/types/chunk'
 
-import { CompletionsParams, CompletionsResult, GenericChunk } from '../schemas'
-import { CompletionsContext, CompletionsMiddleware } from '../types'
+import type { CompletionsParams, CompletionsResult, GenericChunk } from '../schemas'
+import type { CompletionsContext, CompletionsMiddleware } from '../types'
 
 export const MIDDLEWARE_NAME = 'FinalChunkConsumerAndNotifierMiddleware'
 
diff --git a/src/aiCore/legacy/middleware/common/LoggingMiddleware.ts b/src/aiCore/legacy/middleware/common/LoggingMiddleware.ts
index acd371d..480cbbc 100644
--- a/src/aiCore/legacy/middleware/common/LoggingMiddleware.ts
+++ b/src/aiCore/legacy/middleware/common/LoggingMiddleware.ts
@@ -1,6 +1,6 @@
 import { loggerService } from '@logger'
 
-import { BaseContext, MethodMiddleware, MiddlewareAPI } from '../types'
+import type { BaseContext, MethodMiddleware, MiddlewareAPI } from '../types'
 
 const logger = loggerService.withContext('LoggingMiddleware')
 
diff --git a/src/aiCore/legacy/middleware/composer.ts b/src/aiCore/legacy/middleware/composer.ts
index 82b9fd1..97bbf0a 100644
--- a/src/aiCore/legacy/middleware/composer.ts
+++ b/src/aiCore/legacy/middleware/composer.ts
@@ -1,5 +1,5 @@
 import { withSpanResult } from '@renderer/services/SpanManagerService'
-import {
+import type {
   RequestOptions,
   SdkInstance,
   SdkMessageParam,
@@ -10,16 +10,10 @@ import {
   SdkToolCall
 } from '@renderer/types/sdk'
 
-import { BaseApiClient } from '../clients'
-import { CompletionsParams, CompletionsResult } from './schemas'
-import {
-  BaseContext,
-  CompletionsContext,
-  CompletionsMiddleware,
-  MethodMiddleware,
-  MIDDLEWARE_CONTEXT_SYMBOL,
-  MiddlewareAPI
-} from './types'
+import type { BaseApiClient } from '../clients'
+import type { CompletionsParams, CompletionsResult } from './schemas'
+import type { BaseContext, CompletionsContext, CompletionsMiddleware, MethodMiddleware, MiddlewareAPI } from './types'
+import { MIDDLEWARE_CONTEXT_SYMBOL } from './types'
 
 /**
  * Creates the initial context for a method call, populating method-specific fields. /
diff --git a/src/aiCore/legacy/middleware/core/McpToolChunkMiddleware.ts b/src/aiCore/legacy/middleware/core/McpToolChunkMiddleware.ts
index fc03279..6affa5a 100644
--- a/src/aiCore/legacy/middleware/core/McpToolChunkMiddleware.ts
+++ b/src/aiCore/legacy/middleware/core/McpToolChunkMiddleware.ts
@@ -1,7 +1,8 @@
 import { loggerService } from '@logger'
-import { MCPCallToolResponse, MCPTool, MCPToolResponse, Model } from '@renderer/types'
-import { ChunkType, MCPToolCreatedChunk } from '@renderer/types/chunk'
-import { SdkMessageParam, SdkRawOutput, SdkToolCall } from '@renderer/types/sdk'
+import type { MCPCallToolResponse, MCPTool, MCPToolResponse, Model } from '@renderer/types'
+import type { MCPToolCreatedChunk } from '@renderer/types/chunk'
+import { ChunkType } from '@renderer/types/chunk'
+import type { SdkMessageParam, SdkRawOutput, SdkToolCall } from '@renderer/types/sdk'
 import {
   callBuiltInTool,
   callMCPTool,
@@ -12,8 +13,8 @@ import {
 } from '@renderer/utils/mcp-tools'
 import { confirmSameNameTools, requestToolConfirmation, setToolIdToNameMapping } from '@renderer/utils/userConfirmation'
 
-import { CompletionsParams, CompletionsResult, GenericChunk } from '../schemas'
-import { CompletionsContext, CompletionsMiddleware } from '../types'
+import type { CompletionsParams, CompletionsResult, GenericChunk } from '../schemas'
+import type { CompletionsContext, CompletionsMiddleware } from '../types'
 
 export const MIDDLEWARE_NAME = 'McpToolChunkMiddleware'
 const MAX_TOOL_RECURSION_DEPTH = 20 // 防止无限递归
diff --git a/src/aiCore/legacy/middleware/core/RawStreamListenerMiddleware.ts b/src/aiCore/legacy/middleware/core/RawStreamListenerMiddleware.ts
index 0d59ad9..04bfd75 100644
--- a/src/aiCore/legacy/middleware/core/RawStreamListenerMiddleware.ts
+++ b/src/aiCore/legacy/middleware/core/RawStreamListenerMiddleware.ts
@@ -1,9 +1,9 @@
 import { AnthropicAPIClient } from '@renderer/aiCore/legacy/clients/anthropic/AnthropicAPIClient'
-import { AnthropicSdkRawChunk, AnthropicSdkRawOutput } from '@renderer/types/sdk'
+import type { AnthropicSdkRawChunk, AnthropicSdkRawOutput } from '@renderer/types/sdk'
 
-import { AnthropicStreamListener } from '../../clients/types'
-import { CompletionsParams, CompletionsResult } from '../schemas'
-import { CompletionsContext, CompletionsMiddleware } from '../types'
+import type { AnthropicStreamListener } from '../../clients/types'
+import type { CompletionsParams, CompletionsResult } from '../schemas'
+import type { CompletionsContext, CompletionsMiddleware } from '../types'
 
 export const MIDDLEWARE_NAME = 'RawStreamListenerMiddleware'
 
diff --git a/src/aiCore/legacy/middleware/core/ResponseTransformMiddleware.ts b/src/aiCore/legacy/middleware/core/ResponseTransformMiddleware.ts
index 850da83..bdab7b8 100644
--- a/src/aiCore/legacy/middleware/core/ResponseTransformMiddleware.ts
+++ b/src/aiCore/legacy/middleware/core/ResponseTransformMiddleware.ts
@@ -1,9 +1,9 @@
 import { loggerService } from '@logger'
-import { SdkRawChunk } from '@renderer/types/sdk'
+import type { SdkRawChunk } from '@renderer/types/sdk'
 
-import { ResponseChunkTransformerContext } from '../../clients/types'
-import { CompletionsParams, CompletionsResult, GenericChunk } from '../schemas'
-import { CompletionsContext, CompletionsMiddleware } from '../types'
+import type { ResponseChunkTransformerContext } from '../../clients/types'
+import type { CompletionsParams, CompletionsResult, GenericChunk } from '../schemas'
+import type { CompletionsContext, CompletionsMiddleware } from '../types'
 
 export const MIDDLEWARE_NAME = 'ResponseTransformMiddleware'
 
diff --git a/src/aiCore/legacy/middleware/core/StreamAdapterMiddleware.ts b/src/aiCore/legacy/middleware/core/StreamAdapterMiddleware.ts
index 8bb5266..b6dc13e 100644
--- a/src/aiCore/legacy/middleware/core/StreamAdapterMiddleware.ts
+++ b/src/aiCore/legacy/middleware/core/StreamAdapterMiddleware.ts
@@ -1,8 +1,8 @@
-import { SdkRawChunk } from '@renderer/types/sdk'
+import type { SdkRawChunk } from '@renderer/types/sdk'
 import { asyncGeneratorToReadableStream, createSingleChunkReadableStream } from '@renderer/utils/stream'
 
-import { CompletionsParams, CompletionsResult } from '../schemas'
-import { CompletionsContext, CompletionsMiddleware } from '../types'
+import type { CompletionsParams, CompletionsResult } from '../schemas'
+import type { CompletionsContext, CompletionsMiddleware } from '../types'
 import { isAsyncIterable } from '../utils'
 
 export const MIDDLEWARE_NAME = 'StreamAdapterMiddleware'
diff --git a/src/aiCore/legacy/middleware/core/TextChunkMiddleware.ts b/src/aiCore/legacy/middleware/core/TextChunkMiddleware.ts
index 41157bf..8372449 100644
--- a/src/aiCore/legacy/middleware/core/TextChunkMiddleware.ts
+++ b/src/aiCore/legacy/middleware/core/TextChunkMiddleware.ts
@@ -1,8 +1,8 @@
 import { loggerService } from '@logger'
 import { ChunkType } from '@renderer/types/chunk'
 
-import { CompletionsParams, CompletionsResult, GenericChunk } from '../schemas'
-import { CompletionsContext, CompletionsMiddleware } from '../types'
+import type { CompletionsParams, CompletionsResult, GenericChunk } from '../schemas'
+import type { CompletionsContext, CompletionsMiddleware } from '../types'
 
 export const MIDDLEWARE_NAME = 'TextChunkMiddleware'
 
diff --git a/src/aiCore/legacy/middleware/core/ThinkChunkMiddleware.ts b/src/aiCore/legacy/middleware/core/ThinkChunkMiddleware.ts
index 2149d8f..5920cdc 100644
--- a/src/aiCore/legacy/middleware/core/ThinkChunkMiddleware.ts
+++ b/src/aiCore/legacy/middleware/core/ThinkChunkMiddleware.ts
@@ -1,8 +1,9 @@
 import { loggerService } from '@logger'
-import { ChunkType, ThinkingCompleteChunk, ThinkingDeltaChunk } from '@renderer/types/chunk'
+import type { ThinkingCompleteChunk, ThinkingDeltaChunk } from '@renderer/types/chunk'
+import { ChunkType } from '@renderer/types/chunk'
 
-import { CompletionsParams, CompletionsResult, GenericChunk } from '../schemas'
-import { CompletionsContext, CompletionsMiddleware } from '../types'
+import type { CompletionsParams, CompletionsResult, GenericChunk } from '../schemas'
+import type { CompletionsContext, CompletionsMiddleware } from '../types'
 
 export const MIDDLEWARE_NAME = 'ThinkChunkMiddleware'
 
diff --git a/src/aiCore/legacy/middleware/core/TransformCoreToSdkParamsMiddleware.ts b/src/aiCore/legacy/middleware/core/TransformCoreToSdkParamsMiddleware.ts
index 71831a3..ebc86f5 100644
--- a/src/aiCore/legacy/middleware/core/TransformCoreToSdkParamsMiddleware.ts
+++ b/src/aiCore/legacy/middleware/core/TransformCoreToSdkParamsMiddleware.ts
@@ -1,8 +1,8 @@
 import { loggerService } from '@logger'
 import { ChunkType } from '@renderer/types/chunk'
 
-import { CompletionsParams, CompletionsResult } from '../schemas'
-import { CompletionsContext, CompletionsMiddleware } from '../types'
+import type { CompletionsParams, CompletionsResult } from '../schemas'
+import type { CompletionsContext, CompletionsMiddleware } from '../types'
 
 export const MIDDLEWARE_NAME = 'TransformCoreToSdkParamsMiddleware'
 
diff --git a/src/aiCore/legacy/middleware/core/WebSearchMiddleware.ts b/src/aiCore/legacy/middleware/core/WebSearchMiddleware.ts
index ae346af..3365b16 100644
--- a/src/aiCore/legacy/middleware/core/WebSearchMiddleware.ts
+++ b/src/aiCore/legacy/middleware/core/WebSearchMiddleware.ts
@@ -2,8 +2,8 @@ import { loggerService } from '@logger'
 import { ChunkType } from '@renderer/types/chunk'
 import { convertLinks, flushLinkConverterBuffer } from '@renderer/utils/linkConverter'
 
-import { CompletionsParams, CompletionsResult, GenericChunk } from '../schemas'
-import { CompletionsContext, CompletionsMiddleware } from '../types'
+import type { CompletionsParams, CompletionsResult, GenericChunk } from '../schemas'
+import type { CompletionsContext, CompletionsMiddleware } from '../types'
 
 const logger = loggerService.withContext('WebSearchMiddleware')
 
diff --git a/src/aiCore/legacy/middleware/feat/ImageGenerationMiddleware.ts b/src/aiCore/legacy/middleware/feat/ImageGenerationMiddleware.ts
index 40ab43c..0df303e 100644
--- a/src/aiCore/legacy/middleware/feat/ImageGenerationMiddleware.ts
+++ b/src/aiCore/legacy/middleware/feat/ImageGenerationMiddleware.ts
@@ -1,4 +1,4 @@
-import OpenAI from '@cherrystudio/openai'
+import type OpenAI from '@cherrystudio/openai'
 import { toFile } from '@cherrystudio/openai/uploads'
 import { isDedicatedImageGenerationModel } from '@renderer/config/models'
 import FileManager from '@renderer/services/FileManager'
@@ -6,9 +6,9 @@ import { ChunkType } from '@renderer/types/chunk'
 import { findImageBlocks, getMainTextContent } from '@renderer/utils/messageUtils/find'
 import { defaultTimeout } from '@shared/config/constant'
 
-import { BaseApiClient } from '../../clients/BaseApiClient'
-import { CompletionsParams, CompletionsResult, GenericChunk } from '../schemas'
-import { CompletionsContext, CompletionsMiddleware } from '../types'
+import type { BaseApiClient } from '../../clients/BaseApiClient'
+import type { CompletionsParams, CompletionsResult, GenericChunk } from '../schemas'
+import type { CompletionsContext, CompletionsMiddleware } from '../types'
 
 export const MIDDLEWARE_NAME = 'ImageGenerationMiddleware'
 
diff --git a/src/aiCore/legacy/middleware/feat/ThinkingTagExtractionMiddleware.ts b/src/aiCore/legacy/middleware/feat/ThinkingTagExtractionMiddleware.ts
index 447b9d2..dea679e 100644
--- a/src/aiCore/legacy/middleware/feat/ThinkingTagExtractionMiddleware.ts
+++ b/src/aiCore/legacy/middleware/feat/ThinkingTagExtractionMiddleware.ts
@@ -1,17 +1,18 @@
 import { loggerService } from '@logger'
-import { Model } from '@renderer/types'
-import {
-  ChunkType,
+import type { Model } from '@renderer/types'
+import type {
   TextDeltaChunk,
   ThinkingCompleteChunk,
   ThinkingDeltaChunk,
   ThinkingStartChunk
 } from '@renderer/types/chunk'
+import { ChunkType } from '@renderer/types/chunk'
 import { getLowerBaseModelName } from '@renderer/utils'
-import { TagConfig, TagExtractor } from '@renderer/utils/tagExtraction'
+import type { TagConfig } from '@renderer/utils/tagExtraction'
+import { TagExtractor } from '@renderer/utils/tagExtraction'
 
-import { CompletionsParams, CompletionsResult, GenericChunk } from '../schemas'
-import { CompletionsContext, CompletionsMiddleware } from '../types'
+import type { CompletionsParams, CompletionsResult, GenericChunk } from '../schemas'
+import type { CompletionsContext, CompletionsMiddleware } from '../types'
 
 const logger = loggerService.withContext('ThinkingTagExtractionMiddleware')
 
diff --git a/src/aiCore/legacy/middleware/feat/ToolUseExtractionMiddleware.ts b/src/aiCore/legacy/middleware/feat/ToolUseExtractionMiddleware.ts
index 1f559bf..38d842e 100644
--- a/src/aiCore/legacy/middleware/feat/ToolUseExtractionMiddleware.ts
+++ b/src/aiCore/legacy/middleware/feat/ToolUseExtractionMiddleware.ts
@@ -1,11 +1,13 @@
 import { loggerService } from '@logger'
-import { MCPTool } from '@renderer/types'
-import { ChunkType, MCPToolCreatedChunk, TextDeltaChunk } from '@renderer/types/chunk'
+import type { MCPTool } from '@renderer/types'
+import type { MCPToolCreatedChunk, TextDeltaChunk } from '@renderer/types/chunk'
+import { ChunkType } from '@renderer/types/chunk'
 import { parseToolUse } from '@renderer/utils/mcp-tools'
-import { TagConfig, TagExtractor } from '@renderer/utils/tagExtraction'
+import type { TagConfig } from '@renderer/utils/tagExtraction'
+import { TagExtractor } from '@renderer/utils/tagExtraction'
 
-import { CompletionsParams, CompletionsResult, GenericChunk } from '../schemas'
-import { CompletionsContext, CompletionsMiddleware } from '../types'
+import type { CompletionsParams, CompletionsResult, GenericChunk } from '../schemas'
+import type { CompletionsContext, CompletionsMiddleware } from '../types'
 
 export const MIDDLEWARE_NAME = 'ToolUseExtractionMiddleware'
 
diff --git a/src/aiCore/legacy/middleware/index.ts b/src/aiCore/legacy/middleware/index.ts
index 64be4ed..66213c3 100644
--- a/src/aiCore/legacy/middleware/index.ts
+++ b/src/aiCore/legacy/middleware/index.ts
@@ -1,4 +1,4 @@
-import { CompletionsMiddleware, MethodMiddleware } from './types'
+import type { CompletionsMiddleware, MethodMiddleware } from './types'
 
 // /**
 //  * Wraps a provider instance with middlewares.
diff --git a/src/aiCore/legacy/middleware/schemas.ts b/src/aiCore/legacy/middleware/schemas.ts
index ce89934..9119d81 100644
--- a/src/aiCore/legacy/middleware/schemas.ts
+++ b/src/aiCore/legacy/middleware/schemas.ts
@@ -1,9 +1,9 @@
-import { Assistant, MCPTool } from '@renderer/types'
-import { Chunk } from '@renderer/types/chunk'
-import { Message } from '@renderer/types/newMessage'
-import { SdkRawChunk, SdkRawOutput } from '@renderer/types/sdk'
+import type { Assistant, MCPTool } from '@renderer/types'
+import type { Chunk } from '@renderer/types/chunk'
+import type { Message } from '@renderer/types/newMessage'
+import type { SdkRawChunk, SdkRawOutput } from '@renderer/types/sdk'
 
-import { ProcessingState } from './types'
+import type { ProcessingState } from './types'
 
 // ============================================================================
 // Core Request Types - 核心请求结构
diff --git a/src/aiCore/legacy/middleware/types.ts b/src/aiCore/legacy/middleware/types.ts
index 0a7dbe3..3762035 100644
--- a/src/aiCore/legacy/middleware/types.ts
+++ b/src/aiCore/legacy/middleware/types.ts
@@ -1,6 +1,6 @@
-import { MCPToolResponse, Metrics, Usage, WebSearchResponse } from '@renderer/types'
-import { Chunk, ErrorChunk } from '@renderer/types/chunk'
-import {
+import type { MCPToolResponse, Metrics, Usage, WebSearchResponse } from '@renderer/types'
+import type { Chunk, ErrorChunk } from '@renderer/types/chunk'
+import type {
   SdkInstance,
   SdkMessageParam,
   SdkParams,
@@ -10,8 +10,8 @@ import {
   SdkToolCall
 } from '@renderer/types/sdk'
 
-import { BaseApiClient } from '../clients'
-import { CompletionsParams, CompletionsResult } from './schemas'
+import type { BaseApiClient } from '../clients'
+import type { CompletionsParams, CompletionsResult } from './schemas'
 
 /**
  * Symbol to uniquely identify middleware context objects.
diff --git a/src/aiCore/legacy/middleware/utils.ts b/src/aiCore/legacy/middleware/utils.ts
index 12a2fe6..32e94e1 100644
--- a/src/aiCore/legacy/middleware/utils.ts
+++ b/src/aiCore/legacy/middleware/utils.ts
@@ -1,4 +1,5 @@
-import { ChunkType, ErrorChunk } from '@renderer/types/chunk'
+import type { ErrorChunk } from '@renderer/types/chunk'
+import { ChunkType } from '@renderer/types/chunk'
 
 /**
  * Creates an ErrorChunk object with a standardized structure.
diff --git a/src/aiCore/middleware/AiSdkMiddlewareBuilder.ts b/src/aiCore/middleware/AiSdkMiddlewareBuilder.ts
index b10607b..3f14917 100644
--- a/src/aiCore/middleware/AiSdkMiddlewareBuilder.ts
+++ b/src/aiCore/middleware/AiSdkMiddlewareBuilder.ts
@@ -1,10 +1,12 @@
-import { WebSearchPluginConfig } from '@cherrystudio/ai-core/built-in/plugins'
+import type { WebSearchPluginConfig } from '@cherrystudio/ai-core/built-in/plugins'
 import { loggerService } from '@logger'
 import { isSupportedThinkingTokenQwenModel } from '@renderer/config/models'
 import { isSupportEnableThinkingProvider } from '@renderer/config/providers'
-import { type Assistant, MCPTool, type Message, type Model, type Provider } from '@renderer/types'
+import type { MCPTool } from '@renderer/types'
+import { type Assistant, type Message, type Model, type Provider } from '@renderer/types'
 import type { Chunk } from '@renderer/types/chunk'
-import { extractReasoningMiddleware, LanguageModelMiddleware, simulateStreamingMiddleware } from 'ai'
+import type { LanguageModelMiddleware } from 'ai'
+import { extractReasoningMiddleware, simulateStreamingMiddleware } from 'ai'
 import { isEmpty } from 'lodash'
 
 import { isOpenRouterGeminiGenerateImageModel } from '../utils/image'
diff --git a/src/aiCore/middleware/noThinkMiddleware.ts b/src/aiCore/middleware/noThinkMiddleware.ts
index 9d7d933..3e56249 100644
--- a/src/aiCore/middleware/noThinkMiddleware.ts
+++ b/src/aiCore/middleware/noThinkMiddleware.ts
@@ -1,5 +1,5 @@
 import { loggerService } from '@logger'
-import { LanguageModelMiddleware } from 'ai'
+import type { LanguageModelMiddleware } from 'ai'
 
 const logger = loggerService.withContext('noThinkMiddleware')
 
diff --git a/src/aiCore/middleware/openrouterGenerateImageMiddleware.ts b/src/aiCore/middleware/openrouterGenerateImageMiddleware.ts
index 0110d9a..792192b 100644
--- a/src/aiCore/middleware/openrouterGenerateImageMiddleware.ts
+++ b/src/aiCore/middleware/openrouterGenerateImageMiddleware.ts
@@ -1,4 +1,4 @@
-import { LanguageModelMiddleware } from 'ai'
+import type { LanguageModelMiddleware } from 'ai'
 
 /**
  * Returns a LanguageModelMiddleware that ensures the OpenRouter provider is configured to support both
diff --git a/src/aiCore/middleware/qwenThinkingMiddleware.ts b/src/aiCore/middleware/qwenThinkingMiddleware.ts
index 34515a4..931831a 100644
--- a/src/aiCore/middleware/qwenThinkingMiddleware.ts
+++ b/src/aiCore/middleware/qwenThinkingMiddleware.ts
@@ -1,4 +1,4 @@
-import { LanguageModelMiddleware } from 'ai'
+import type { LanguageModelMiddleware } from 'ai'
 
 /**
  * Qwen Thinking Middleware
diff --git a/src/aiCore/middleware/toolChoiceMiddleware.ts b/src/aiCore/middleware/toolChoiceMiddleware.ts
index 6d3ba37..7bb00af 100644
--- a/src/aiCore/middleware/toolChoiceMiddleware.ts
+++ b/src/aiCore/middleware/toolChoiceMiddleware.ts
@@ -1,5 +1,5 @@
 import { loggerService } from '@logger'
-import { LanguageModelMiddleware } from 'ai'
+import type { LanguageModelMiddleware } from 'ai'
 
 const logger = loggerService.withContext('toolChoiceMiddleware')
 
diff --git a/src/aiCore/plugins/PluginBuilder.ts b/src/aiCore/plugins/PluginBuilder.ts
index c249142..eb46eb7 100644
--- a/src/aiCore/plugins/PluginBuilder.ts
+++ b/src/aiCore/plugins/PluginBuilder.ts
@@ -1,10 +1,10 @@
-import { AiPlugin } from '@cherrystudio/ai-core'
+import type { AiPlugin } from '@cherrystudio/ai-core'
 import { createPromptToolUsePlugin, webSearchPlugin } from '@cherrystudio/ai-core/built-in/plugins'
 import { loggerService } from '@logger'
 import { getEnableDeveloperMode } from '@renderer/hooks/useSettings'
-import { Assistant } from '@renderer/types'
+import type { Assistant } from '@renderer/types'
 
-import { AiSdkMiddlewareConfig } from '../middleware/AiSdkMiddlewareBuilder'
+import type { AiSdkMiddlewareConfig } from '../middleware/AiSdkMiddlewareBuilder'
 import { searchOrchestrationPlugin } from './searchOrchestrationPlugin'
 import { createTelemetryPlugin } from './telemetryPlugin'
 
diff --git a/src/aiCore/plugins/searchOrchestrationPlugin.ts b/src/aiCore/plugins/searchOrchestrationPlugin.ts
index 7e662dd..6be577f 100644
--- a/src/aiCore/plugins/searchOrchestrationPlugin.ts
+++ b/src/aiCore/plugins/searchOrchestrationPlugin.ts
@@ -18,7 +18,8 @@ import { getDefaultModel, getProviderByModel } from '@renderer/services/Assistan
 import store from '@renderer/store'
 import { selectCurrentUserId, selectGlobalMemoryEnabled, selectMemoryConfig } from '@renderer/store/memory'
 import type { Assistant } from '@renderer/types'
-import { extractInfoFromXML, ExtractResults } from '@renderer/utils/extract'
+import type { ExtractResults } from '@renderer/utils/extract'
+import { extractInfoFromXML } from '@renderer/utils/extract'
 import type { LanguageModel, ModelMessage } from 'ai'
 import { generateText } from 'ai'
 import { isEmpty } from 'lodash'
diff --git a/src/aiCore/plugins/telemetryPlugin.ts b/src/aiCore/plugins/telemetryPlugin.ts
index 0f06091..485d339 100644
--- a/src/aiCore/plugins/telemetryPlugin.ts
+++ b/src/aiCore/plugins/telemetryPlugin.ts
@@ -8,10 +8,11 @@
 
 import { definePlugin } from '@cherrystudio/ai-core'
 import { loggerService } from '@logger'
-import { Context, context as otelContext, Span, SpanContext, trace, Tracer } from '@opentelemetry/api'
+import type { Context, Span, SpanContext, Tracer } from '@opentelemetry/api'
+import { context as otelContext, trace } from '@opentelemetry/api'
 import { currentSpan } from '@renderer/services/SpanManagerService'
 import { webTraceService } from '@renderer/services/WebTraceService'
-import { Assistant } from '@renderer/types'
+import type { Assistant } from '@renderer/types'
 
 import { AiSdkSpanAdapter } from '../trace/AiSdkSpanAdapter'
 
diff --git a/src/aiCore/prepareParams/fileProcessor.ts b/src/aiCore/prepareParams/fileProcessor.ts
index 4904812..5ee8812 100644
--- a/src/aiCore/prepareParams/fileProcessor.ts
+++ b/src/aiCore/prepareParams/fileProcessor.ts
@@ -8,7 +8,7 @@ import { loggerService } from '@logger'
 import { getProviderByModel } from '@renderer/services/AssistantService'
 import type { FileMetadata, Message, Model } from '@renderer/types'
 import { FileTypes } from '@renderer/types'
-import { FileMessageBlock } from '@renderer/types/newMessage'
+import type { FileMessageBlock } from '@renderer/types/newMessage'
 import { findFileBlocks } from '@renderer/utils/messageUtils/find'
 import type { FilePart, TextPart } from 'ai'
 
diff --git a/src/aiCore/prepareParams/messageConverter.ts b/src/aiCore/prepareParams/messageConverter.ts
index 46cacb5..72f387d 100644
--- a/src/aiCore/prepareParams/messageConverter.ts
+++ b/src/aiCore/prepareParams/messageConverter.ts
@@ -6,7 +6,7 @@
 import { loggerService } from '@logger'
 import { isImageEnhancementModel, isVisionModel } from '@renderer/config/models'
 import type { Message, Model } from '@renderer/types'
-import { FileMessageBlock, ImageMessageBlock, ThinkingMessageBlock } from '@renderer/types/newMessage'
+import type { FileMessageBlock, ImageMessageBlock, ThinkingMessageBlock } from '@renderer/types/newMessage'
 import {
   findFileBlocks,
   findImageBlocks,
diff --git a/src/aiCore/prepareParams/modelCapabilities.ts b/src/aiCore/prepareParams/modelCapabilities.ts
index 4a3c3f4..b6e4b25 100644
--- a/src/aiCore/prepareParams/modelCapabilities.ts
+++ b/src/aiCore/prepareParams/modelCapabilities.ts
@@ -85,6 +85,19 @@ export function supportsLargeFileUpload(model: Model): boolean {
   })
 }
 
+/**
+ * 检查模型是否支持TopP
+ */
+export function supportsTopP(model: Model): boolean {
+  const provider = getProviderByModel(model)
+
+  if (provider?.type === 'anthropic' || model?.endpoint_type === 'anthropic') {
+    return false
+  }
+
+  return true
+}
+
 /**
  * 获取提供商特定的文件大小限制
  */
diff --git a/src/aiCore/prepareParams/parameterBuilder.ts b/src/aiCore/prepareParams/parameterBuilder.ts
index c693ed2..397c481 100644
--- a/src/aiCore/prepareParams/parameterBuilder.ts
+++ b/src/aiCore/prepareParams/parameterBuilder.ts
@@ -7,7 +7,7 @@ import { anthropic } from '@ai-sdk/anthropic'
 import { google } from '@ai-sdk/google'
 import { vertexAnthropic } from '@ai-sdk/google-vertex/anthropic/edge'
 import { vertex } from '@ai-sdk/google-vertex/edge'
-import { WebSearchPluginConfig } from '@cherrystudio/ai-core/built-in/plugins'
+import type { WebSearchPluginConfig } from '@cherrystudio/ai-core/built-in/plugins'
 import { isBaseProvider } from '@cherrystudio/ai-core/core/providers/schemas'
 import { loggerService } from '@logger'
 import {
@@ -21,7 +21,7 @@ import {
 } from '@renderer/config/models'
 import { getAssistantSettings, getDefaultModel } from '@renderer/services/AssistantService'
 import store from '@renderer/store'
-import { CherryWebSearchConfig } from '@renderer/store/websearch'
+import type { CherryWebSearchConfig } from '@renderer/store/websearch'
 import { type Assistant, type MCPTool, type Provider } from '@renderer/types'
 import type { StreamTextParams } from '@renderer/types/aiCoreTypes'
 import { mapRegexToPatterns } from '@renderer/utils/blacklistMatchPattern'
@@ -34,6 +34,7 @@ import { setupToolsConfig } from '../utils/mcp'
 import { buildProviderOptions } from '../utils/options'
 import { getAnthropicThinkingBudget } from '../utils/reasoning'
 import { buildProviderBuiltinWebSearchConfig } from '../utils/websearch'
+import { supportsTopP } from './modelCapabilities'
 import { getTemperature, getTopP } from './modelParameters'
 
 const logger = loggerService.withContext('parameterBuilder')
@@ -176,20 +177,27 @@ export async function buildStreamTextParams(
     messages: sdkMessages,
     maxOutputTokens: maxTokens,
     temperature: getTemperature(assistant, model),
-    topP: getTopP(assistant, model),
     abortSignal: options.requestOptions?.signal,
     headers: options.requestOptions?.headers,
     providerOptions,
     stopWhen: stepCountIs(20),
     maxRetries: 0
   }
+
+  if (supportsTopP(model)) {
+    params.topP = getTopP(assistant, model)
+  }
+
   if (tools) {
     params.tools = tools
   }
+
   if (assistant.prompt) {
     params.system = await replacePromptVariables(assistant.prompt, model.name)
   }
+
   logger.debug('params', params)
+
   return {
     params,
     modelId: model.id,
diff --git a/src/aiCore/provider/__tests__/providerConfig.test.ts b/src/aiCore/provider/__tests__/providerConfig.test.ts
index eb6e73c..3978623 100644
--- a/src/aiCore/provider/__tests__/providerConfig.test.ts
+++ b/src/aiCore/provider/__tests__/providerConfig.test.ts
@@ -21,10 +21,45 @@ vi.mock('@renderer/store', () => ({
   }
 }))
 
+vi.mock('@renderer/utils/api', () => ({
+  formatApiHost: vi.fn((host, isSupportedAPIVersion = true) => {
+    if (isSupportedAPIVersion === false) {
+      return host // Return host as-is when isSupportedAPIVersion is false
+    }
+    return `${host}/v1` // Default behavior when isSupportedAPIVersion is true
+  }),
+  routeToEndpoint: vi.fn((host) => ({
+    baseURL: host,
+    endpoint: '/chat/completions'
+  }))
+}))
+
+vi.mock('@renderer/config/providers', async (importOriginal) => {
+  const actual = (await importOriginal()) as any
+  return {
+    ...actual,
+    isCherryAIProvider: vi.fn(),
+    isPerplexityProvider: vi.fn(),
+    isAnthropicProvider: vi.fn(() => false),
+    isAzureOpenAIProvider: vi.fn(() => false),
+    isGeminiProvider: vi.fn(() => false),
+    isNewApiProvider: vi.fn(() => false)
+  }
+})
+
+vi.mock('@renderer/hooks/useVertexAI', () => ({
+  isVertexProvider: vi.fn(() => false),
+  isVertexAIConfigured: vi.fn(() => false),
+  createVertexProvider: vi.fn()
+}))
+
+import { isCherryAIProvider, isPerplexityProvider } from '@renderer/config/providers'
+import { getProviderByModel } from '@renderer/services/AssistantService'
 import type { Model, Provider } from '@renderer/types'
+import { formatApiHost } from '@renderer/utils/api'
 
 import { COPILOT_DEFAULT_HEADERS, COPILOT_EDITOR_VERSION, isCopilotResponsesModel } from '../constants'
-import { providerToAiSdkConfig } from '../providerConfig'
+import { getActualProvider, providerToAiSdkConfig } from '../providerConfig'
 
 const createWindowKeyv = () => {
   const store = new Map<string, string>()
@@ -46,11 +81,31 @@ const createCopilotProvider = (): Provider => ({
   isSystem: true
 })
 
-const createModel = (id: string, name = id): Model => ({
+const createModel = (id: string, name = id, provider = 'copilot'): Model => ({
   id,
   name,
-  provider: 'copilot',
-  group: 'copilot'
+  provider,
+  group: provider
+})
+
+const createCherryAIProvider = (): Provider => ({
+  id: 'cherryai',
+  type: 'openai',
+  name: 'CherryAI',
+  apiKey: 'test-key',
+  apiHost: 'https://api.cherryai.com',
+  models: [],
+  isSystem: false
+})
+
+const createPerplexityProvider = (): Provider => ({
+  id: 'perplexity',
+  type: 'openai',
+  name: 'Perplexity',
+  apiKey: 'test-key',
+  apiHost: 'https://api.perplexity.ai',
+  models: [],
+  isSystem: false
 })
 
 describe('Copilot responses routing', () => {
@@ -87,3 +142,134 @@ describe('Copilot responses routing', () => {
     expect(config.options.headers?.['Copilot-Integration-Id']).toBe(COPILOT_DEFAULT_HEADERS['Copilot-Integration-Id'])
   })
 })
+
+describe('CherryAI provider configuration', () => {
+  beforeEach(() => {
+    ;(globalThis as any).window = {
+      ...(globalThis as any).window,
+      keyv: createWindowKeyv()
+    }
+    vi.clearAllMocks()
+  })
+
+  it('formats CherryAI provider apiHost with false parameter', () => {
+    const provider = createCherryAIProvider()
+    const model = createModel('gpt-4', 'GPT-4', 'cherryai')
+
+    // Mock the functions to simulate CherryAI provider detection
+    vi.mocked(isCherryAIProvider).mockReturnValue(true)
+    vi.mocked(getProviderByModel).mockReturnValue(provider)
+
+    // Call getActualProvider which should trigger formatProviderApiHost
+    const actualProvider = getActualProvider(model)
+
+    // Verify that formatApiHost was called with false as the second parameter
+    expect(formatApiHost).toHaveBeenCalledWith('https://api.cherryai.com', false)
+    expect(actualProvider.apiHost).toBe('https://api.cherryai.com')
+  })
+
+  it('does not format non-CherryAI provider with false parameter', () => {
+    const provider = {
+      id: 'openai',
+      type: 'openai',
+      name: 'OpenAI',
+      apiKey: 'test-key',
+      apiHost: 'https://api.openai.com',
+      models: [],
+      isSystem: false
+    } as Provider
+    const model = createModel('gpt-4', 'GPT-4', 'openai')
+
+    // Mock the functions to simulate non-CherryAI provider
+    vi.mocked(isCherryAIProvider).mockReturnValue(false)
+    vi.mocked(getProviderByModel).mockReturnValue(provider)
+
+    // Call getActualProvider
+    const actualProvider = getActualProvider(model)
+
+    // Verify that formatApiHost was called with default parameters (true)
+    expect(formatApiHost).toHaveBeenCalledWith('https://api.openai.com')
+    expect(actualProvider.apiHost).toBe('https://api.openai.com/v1')
+  })
+
+  it('handles CherryAI provider with empty apiHost', () => {
+    const provider = createCherryAIProvider()
+    provider.apiHost = ''
+    const model = createModel('gpt-4', 'GPT-4', 'cherryai')
+
+    vi.mocked(isCherryAIProvider).mockReturnValue(true)
+    vi.mocked(getProviderByModel).mockReturnValue(provider)
+
+    const actualProvider = getActualProvider(model)
+
+    expect(formatApiHost).toHaveBeenCalledWith('', false)
+    expect(actualProvider.apiHost).toBe('')
+  })
+})
+
+describe('Perplexity provider configuration', () => {
+  beforeEach(() => {
+    ;(globalThis as any).window = {
+      ...(globalThis as any).window,
+      keyv: createWindowKeyv()
+    }
+    vi.clearAllMocks()
+  })
+
+  it('formats Perplexity provider apiHost with false parameter', () => {
+    const provider = createPerplexityProvider()
+    const model = createModel('sonar', 'Sonar', 'perplexity')
+
+    // Mock the functions to simulate Perplexity provider detection
+    vi.mocked(isCherryAIProvider).mockReturnValue(false)
+    vi.mocked(isPerplexityProvider).mockReturnValue(true)
+    vi.mocked(getProviderByModel).mockReturnValue(provider)
+
+    // Call getActualProvider which should trigger formatProviderApiHost
+    const actualProvider = getActualProvider(model)
+
+    // Verify that formatApiHost was called with false as the second parameter
+    expect(formatApiHost).toHaveBeenCalledWith('https://api.perplexity.ai', false)
+    expect(actualProvider.apiHost).toBe('https://api.perplexity.ai')
+  })
+
+  it('does not format non-Perplexity provider with false parameter', () => {
+    const provider = {
+      id: 'openai',
+      type: 'openai',
+      name: 'OpenAI',
+      apiKey: 'test-key',
+      apiHost: 'https://api.openai.com',
+      models: [],
+      isSystem: false
+    } as Provider
+    const model = createModel('gpt-4', 'GPT-4', 'openai')
+
+    // Mock the functions to simulate non-Perplexity provider
+    vi.mocked(isCherryAIProvider).mockReturnValue(false)
+    vi.mocked(isPerplexityProvider).mockReturnValue(false)
+    vi.mocked(getProviderByModel).mockReturnValue(provider)
+
+    // Call getActualProvider
+    const actualProvider = getActualProvider(model)
+
+    // Verify that formatApiHost was called with default parameters (true)
+    expect(formatApiHost).toHaveBeenCalledWith('https://api.openai.com')
+    expect(actualProvider.apiHost).toBe('https://api.openai.com/v1')
+  })
+
+  it('handles Perplexity provider with empty apiHost', () => {
+    const provider = createPerplexityProvider()
+    provider.apiHost = ''
+    const model = createModel('sonar', 'Sonar', 'perplexity')
+
+    vi.mocked(isCherryAIProvider).mockReturnValue(false)
+    vi.mocked(isPerplexityProvider).mockReturnValue(true)
+    vi.mocked(getProviderByModel).mockReturnValue(provider)
+
+    const actualProvider = getActualProvider(model)
+
+    expect(formatApiHost).toHaveBeenCalledWith('', false)
+    expect(actualProvider.apiHost).toBe('')
+  })
+})
diff --git a/src/aiCore/provider/config/aihubmix.ts b/src/aiCore/provider/config/aihubmix.ts
index 819e9cd..8feed89 100644
--- a/src/aiCore/provider/config/aihubmix.ts
+++ b/src/aiCore/provider/config/aihubmix.ts
@@ -2,7 +2,7 @@
  * AiHubMix规则集
  */
 import { isOpenAILLMModel } from '@renderer/config/models'
-import { Provider } from '@renderer/types'
+import type { Provider } from '@renderer/types'
 
 import { provider2Provider, startsWith } from './helper'
 import type { RuleSet } from './types'
@@ -52,7 +52,7 @@ const AIHUBMIX_RULES: RuleSet = {
       }
     }
   ],
-  fallbackRule: (provider: Provider) => provider
+  fallbackRule: (provider: Provider) => extraProviderConfig(provider)
 }
 
 export const aihubmixProviderCreator = provider2Provider.bind(null, AIHUBMIX_RULES)
diff --git a/src/aiCore/provider/config/newApi.ts b/src/aiCore/provider/config/newApi.ts
index 5277495..97de625 100644
--- a/src/aiCore/provider/config/newApi.ts
+++ b/src/aiCore/provider/config/newApi.ts
@@ -1,7 +1,7 @@
 /**
  * NewAPI规则集
  */
-import { Provider } from '@renderer/types'
+import type { Provider } from '@renderer/types'
 
 import { endpointIs, provider2Provider } from './helper'
 import type { RuleSet } from './types'
diff --git a/src/aiCore/provider/factory.ts b/src/aiCore/provider/factory.ts
index 6221110..4cdbfb6 100644
--- a/src/aiCore/provider/factory.ts
+++ b/src/aiCore/provider/factory.ts
@@ -1,7 +1,7 @@
 import { hasProviderConfigByAlias, type ProviderId, resolveProviderConfigId } from '@cherrystudio/ai-core/provider'
 import { createProvider as createProviderCore } from '@cherrystudio/ai-core/provider'
 import { loggerService } from '@logger'
-import { Provider } from '@renderer/types'
+import type { Provider } from '@renderer/types'
 import type { Provider as AiSdkProvider } from 'ai'
 
 import { initializeNewProviders } from './providerInitialization'
diff --git a/src/aiCore/provider/providerConfig.ts b/src/aiCore/provider/providerConfig.ts
index 1532375..7f279a3 100644
--- a/src/aiCore/provider/providerConfig.ts
+++ b/src/aiCore/provider/providerConfig.ts
@@ -9,11 +9,15 @@ import { isOpenAIChatCompletionOnlyModel } from '@renderer/config/models'
 import {
   isAnthropicProvider,
   isAzureOpenAIProvider,
+  isCherryAIProvider,
   isGeminiProvider,
-  isNewApiProvider
+  isNewApiProvider,
+  isPerplexityProvider
 } from '@renderer/config/providers'
 import {
   getAwsBedrockAccessKeyId,
+  getAwsBedrockApiKey,
+  getAwsBedrockAuthType,
   getAwsBedrockRegion,
   getAwsBedrockSecretAccessKey
 } from '@renderer/hooks/useAwsBedrock'
@@ -98,6 +102,10 @@ function formatProviderApiHost(provider: Provider): Provider {
     formatted.apiHost = formatAzureOpenAIApiHost(formatted.apiHost)
   } else if (isVertexProvider(formatted)) {
     formatted.apiHost = formatVertexApiHost(formatted)
+  } else if (isCherryAIProvider(formatted)) {
+    formatted.apiHost = formatApiHost(formatted.apiHost, false)
+  } else if (isPerplexityProvider(formatted)) {
+    formatted.apiHost = formatApiHost(formatted.apiHost, false)
   } else {
     formatted.apiHost = formatApiHost(formatted.apiHost)
   }
@@ -192,9 +200,15 @@ export function providerToAiSdkConfig(
 
   // bedrock
   if (aiSdkProviderId === 'bedrock') {
+    const authType = getAwsBedrockAuthType()
     extraOptions.region = getAwsBedrockRegion()
-    extraOptions.accessKeyId = getAwsBedrockAccessKeyId()
-    extraOptions.secretAccessKey = getAwsBedrockSecretAccessKey()
+
+    if (authType === 'apiKey') {
+      extraOptions.apiKey = getAwsBedrockApiKey()
+    } else {
+      extraOptions.accessKeyId = getAwsBedrockAccessKeyId()
+      extraOptions.secretAccessKey = getAwsBedrockSecretAccessKey()
+    }
   }
   // google-vertex
   if (aiSdkProviderId === 'google-vertex' || aiSdkProviderId === 'google-vertex-anthropic') {
diff --git a/src/aiCore/tools/KnowledgeSearchTool.ts b/src/aiCore/tools/KnowledgeSearchTool.ts
index f3a4761..9a1a94f 100644
--- a/src/aiCore/tools/KnowledgeSearchTool.ts
+++ b/src/aiCore/tools/KnowledgeSearchTool.ts
@@ -1,7 +1,7 @@
 import { REFERENCE_PROMPT } from '@renderer/config/prompts'
 import { processKnowledgeSearch } from '@renderer/services/KnowledgeService'
 import type { Assistant, KnowledgeReference } from '@renderer/types'
-import { ExtractResults, KnowledgeExtractResults } from '@renderer/utils/extract'
+import type { ExtractResults, KnowledgeExtractResults } from '@renderer/utils/extract'
 import { type InferToolInput, type InferToolOutput, tool } from 'ai'
 import { isEmpty } from 'lodash'
 import * as z from 'zod'
diff --git a/src/aiCore/tools/WebSearchTool.ts b/src/aiCore/tools/WebSearchTool.ts
index 61d5d3b..9545b64 100644
--- a/src/aiCore/tools/WebSearchTool.ts
+++ b/src/aiCore/tools/WebSearchTool.ts
@@ -1,7 +1,7 @@
 import { REFERENCE_PROMPT } from '@renderer/config/prompts'
 import WebSearchService from '@renderer/services/WebSearchService'
-import { WebSearchProvider, WebSearchProviderResponse } from '@renderer/types'
-import { ExtractResults } from '@renderer/utils/extract'
+import type { WebSearchProvider, WebSearchProviderResponse } from '@renderer/types'
+import type { ExtractResults } from '@renderer/utils/extract'
 import { type InferToolInput, type InferToolOutput, tool } from 'ai'
 import * as z from 'zod'
 
diff --git a/src/aiCore/trace/AiSdkSpanAdapter.ts b/src/aiCore/trace/AiSdkSpanAdapter.ts
index fc844b5..732397d 100644
--- a/src/aiCore/trace/AiSdkSpanAdapter.ts
+++ b/src/aiCore/trace/AiSdkSpanAdapter.ts
@@ -6,8 +6,9 @@
  */
 
 import { loggerService } from '@logger'
-import { SpanEntity, TokenUsage } from '@mcp-trace/trace-core'
-import { Span, SpanKind, SpanStatusCode } from '@opentelemetry/api'
+import type { SpanEntity, TokenUsage } from '@mcp-trace/trace-core'
+import type { Span } from '@opentelemetry/api'
+import { SpanKind, SpanStatusCode } from '@opentelemetry/api'
 
 const logger = loggerService.withContext('AiSdkSpanAdapter')
 
diff --git a/src/aiCore/utils/image.ts b/src/aiCore/utils/image.ts
index 43d9166..37dbe76 100644
--- a/src/aiCore/utils/image.ts
+++ b/src/aiCore/utils/image.ts
@@ -1,4 +1,5 @@
-import { isSystemProvider, Model, Provider, SystemProviderIds } from '@renderer/types'
+import type { Model, Provider } from '@renderer/types'
+import { isSystemProvider, SystemProviderIds } from '@renderer/types'
 
 export function buildGeminiGenerateImageParams(): Record<string, any> {
   return {
diff --git a/src/aiCore/utils/mcp.ts b/src/aiCore/utils/mcp.ts
index 9606d9e..84bc661 100644
--- a/src/aiCore/utils/mcp.ts
+++ b/src/aiCore/utils/mcp.ts
@@ -1,10 +1,10 @@
 import { loggerService } from '@logger'
-import { MCPTool, MCPToolResponse } from '@renderer/types'
+import type { MCPTool, MCPToolResponse } from '@renderer/types'
 import { callMCPTool, getMcpServerByTool, isToolAutoApproved } from '@renderer/utils/mcp-tools'
 import { requestToolConfirmation } from '@renderer/utils/userConfirmation'
 import { type Tool, type ToolSet } from 'ai'
 import { jsonSchema, tool } from 'ai'
-import { JSONSchema7 } from 'json-schema'
+import type { JSONSchema7 } from 'json-schema'
 
 const logger = loggerService.withContext('MCP-utils')
 
diff --git a/src/aiCore/utils/options.ts b/src/aiCore/utils/options.ts
index 087a9ef..60d9b1e 100644
--- a/src/aiCore/utils/options.ts
+++ b/src/aiCore/utils/options.ts
@@ -2,15 +2,13 @@ import { baseProviderIdSchema, customProviderIdSchema } from '@cherrystudio/ai-c
 import { isOpenAIModel, isQwenMTModel, isSupportFlexServiceTierModel } from '@renderer/config/models'
 import { isSupportServiceTierProvider } from '@renderer/config/providers'
 import { mapLanguageToQwenMTModel } from '@renderer/config/translate'
+import type { Assistant, Model, Provider } from '@renderer/types'
 import {
-  Assistant,
   GroqServiceTiers,
   isGroqServiceTier,
   isOpenAIServiceTier,
   isTranslateAssistant,
-  Model,
   OpenAIServiceTiers,
-  Provider,
   SystemProviderIds
 } from '@renderer/types'
 import { t } from 'i18next'
@@ -19,6 +17,7 @@ import { getAiSdkProviderId } from '../provider/factory'
 import { buildGeminiGenerateImageParams } from './image'
 import {
   getAnthropicReasoningParams,
+  getBedrockReasoningParams,
   getCustomParameters,
   getGeminiReasoningParams,
   getOpenAIReasoningParams,
@@ -129,6 +128,9 @@ export function buildProviderOptions(
         case 'google-vertex-anthropic':
           providerSpecificOptions = buildAnthropicProviderOptions(assistant, model, capabilities)
           break
+        case 'bedrock':
+          providerSpecificOptions = buildBedrockProviderOptions(assistant, model, capabilities)
+          break
         default:
           // 对于其他 provider，使用通用的构建逻辑
           providerSpecificOptions = {
@@ -268,6 +270,32 @@ function buildXAIProviderOptions(
   return providerOptions
 }
 
+/**
+ * Build Bedrock providerOptions
+ */
+function buildBedrockProviderOptions(
+  assistant: Assistant,
+  model: Model,
+  capabilities: {
+    enableReasoning: boolean
+    enableWebSearch: boolean
+    enableGenerateImage: boolean
+  }
+): Record<string, any> {
+  const { enableReasoning } = capabilities
+  let providerOptions: Record<string, any> = {}
+
+  if (enableReasoning) {
+    const reasoningParams = getBedrockReasoningParams(assistant, model)
+    providerOptions = {
+      ...providerOptions,
+      ...reasoningParams
+    }
+  }
+
+  return providerOptions
+}
+
 /**
  * 构建通用的 providerOptions（用于其他 provider）
  */
diff --git a/src/aiCore/utils/reasoning.ts b/src/aiCore/utils/reasoning.ts
index 86a7628..3a36fb6 100644
--- a/src/aiCore/utils/reasoning.ts
+++ b/src/aiCore/utils/reasoning.ts
@@ -30,9 +30,10 @@ import {
 import { isSupportEnableThinkingProvider } from '@renderer/config/providers'
 import { getStoreSetting } from '@renderer/hooks/useSettings'
 import { getAssistantSettings, getProviderByModel } from '@renderer/services/AssistantService'
-import { SettingsState } from '@renderer/store/settings'
-import { Assistant, EFFORT_RATIO, isSystemProvider, Model, SystemProviderIds } from '@renderer/types'
-import { ReasoningEffortOptionalParams } from '@renderer/types/sdk'
+import type { SettingsState } from '@renderer/store/settings'
+import type { Assistant, Model } from '@renderer/types'
+import { EFFORT_RATIO, isSystemProvider, SystemProviderIds } from '@renderer/types'
+import type { ReasoningEffortOptionalParams } from '@renderer/types/sdk'
 import { toInteger } from 'lodash'
 
 const logger = loggerService.withContext('reasoning')
@@ -97,7 +98,7 @@ export function getReasoningEffort(assistant: Assistant, model: Model): Reasonin
           extra_body: {
             google: {
               thinking_config: {
-                thinkingBudget: 0
+                thinking_budget: 0
               }
             }
           }
@@ -258,8 +259,8 @@ export function getReasoningEffort(assistant: Assistant, model: Model): Reasonin
         extra_body: {
           google: {
             thinking_config: {
-              thinkingBudget: -1,
-              includeThoughts: true
+              thinking_budget: -1,
+              include_thoughts: true
             }
           }
         }
@@ -269,8 +270,8 @@ export function getReasoningEffort(assistant: Assistant, model: Model): Reasonin
       extra_body: {
         google: {
           thinking_config: {
-            thinkingBudget: budgetTokens,
-            includeThoughts: true
+            thinking_budget: budgetTokens ?? -1,
+            include_thoughts: true
           }
         }
       }
@@ -430,8 +431,8 @@ export function getGeminiReasoningParams(assistant: Assistant, model: Model): Re
     if (reasoningEffort === undefined) {
       return {
         thinkingConfig: {
-          includeThoughts: false,
-          ...(GEMINI_FLASH_MODEL_REGEX.test(model.id) ? { thinkingBudget: 0 } : {})
+          include_thoughts: false,
+          ...(GEMINI_FLASH_MODEL_REGEX.test(model.id) ? { thinking_budget: 0 } : {})
         }
       }
     }
@@ -441,7 +442,7 @@ export function getGeminiReasoningParams(assistant: Assistant, model: Model): Re
     if (effortRatio > 1) {
       return {
         thinkingConfig: {
-          includeThoughts: true
+          include_thoughts: true
         }
       }
     }
@@ -451,8 +452,8 @@ export function getGeminiReasoningParams(assistant: Assistant, model: Model): Re
 
     return {
       thinkingConfig: {
-        ...(budget > 0 ? { thinkingBudget: budget } : {}),
-        includeThoughts: true
+        ...(budget > 0 ? { thinking_budget: budget } : {}),
+        include_thoughts: true
       }
     }
   }
@@ -484,6 +485,34 @@ export function getXAIReasoningParams(assistant: Assistant, model: Model): Recor
   }
 }
 
+/**
+ * Get Bedrock reasoning parameters
+ */
+export function getBedrockReasoningParams(assistant: Assistant, model: Model): Record<string, any> {
+  if (!isReasoningModel(model)) {
+    return {}
+  }
+
+  const reasoningEffort = assistant?.settings?.reasoning_effort
+
+  if (reasoningEffort === undefined) {
+    return {}
+  }
+
+  // Only apply thinking budget for Claude reasoning models
+  if (!isSupportedThinkingTokenClaudeModel(model)) {
+    return {}
+  }
+
+  const budgetTokens = getAnthropicThinkingBudget(assistant, model)
+  return {
+    reasoningConfig: {
+      type: 'enabled',
+      budgetTokens: budgetTokens
+    }
+  }
+}
+
 /**
  * 获取自定义参数
  * 从 assistant 设置中提取自定义参数
diff --git a/src/aiCore/utils/websearch.ts b/src/aiCore/utils/websearch.ts
index 0ab41d5..fde4ff5 100644
--- a/src/aiCore/utils/websearch.ts
+++ b/src/aiCore/utils/websearch.ts
@@ -1,12 +1,12 @@
-import {
+import type {
   AnthropicSearchConfig,
   OpenAISearchConfig,
   WebSearchPluginConfig
 } from '@cherrystudio/ai-core/core/plugins/built-in/webSearchPlugin/helper'
-import { BaseProviderId } from '@cherrystudio/ai-core/provider'
+import type { BaseProviderId } from '@cherrystudio/ai-core/provider'
 import { isOpenAIDeepResearchModel, isOpenAIWebSearchChatCompletionOnlyModel } from '@renderer/config/models'
-import { CherryWebSearchConfig } from '@renderer/store/websearch'
-import { Model } from '@renderer/types'
+import type { CherryWebSearchConfig } from '@renderer/store/websearch'
+import type { Model } from '@renderer/types'
 import { mapRegexToPatterns } from '@renderer/utils/blacklistMatchPattern'
 
 export function getWebSearchParams(model: Model): Record<string, any> {
