diff --git a/src/aiCore/chunk/AiSdkToChunkAdapter.ts b/src/aiCore/chunk/AiSdkToChunkAdapter.ts
index 6e4288d..544ec44 100644
--- a/src/aiCore/chunk/AiSdkToChunkAdapter.ts
+++ b/src/aiCore/chunk/AiSdkToChunkAdapter.ts
@@ -30,18 +30,22 @@ export class AiSdkToChunkAdapter {
   private onSessionUpdate?: (sessionId: string) => void
   private responseStartTimestamp: number | null = null
   private firstTokenTimestamp: number | null = null
+  private hasTextContent = false
+  private getSessionWasCleared?: () => boolean
 
   constructor(
     private onChunk: (chunk: Chunk) => void,
     mcpTools: MCPTool[] = [],
     accumulate?: boolean,
     enableWebSearch?: boolean,
-    onSessionUpdate?: (sessionId: string) => void
+    onSessionUpdate?: (sessionId: string) => void,
+    getSessionWasCleared?: () => boolean
   ) {
     this.toolCallHandler = new ToolCallChunkHandler(onChunk, mcpTools)
     this.accumulate = accumulate
     this.enableWebSearch = enableWebSearch || false
     this.onSessionUpdate = onSessionUpdate
+    this.getSessionWasCleared = getSessionWasCleared
   }
 
   private markFirstTokenIfNeeded() {
@@ -84,8 +88,9 @@ export class AiSdkToChunkAdapter {
     }
     this.resetTimingState()
     this.responseStartTimestamp = Date.now()
-    // Reset link converter state at the start of stream
+    // Reset state at the start of stream
     this.isFirstChunk = true
+    this.hasTextContent = false
 
     try {
       while (true) {
@@ -129,6 +134,8 @@ export class AiSdkToChunkAdapter {
         const agentRawMessage = chunk.rawValue as ClaudeCodeRawValue
         if (agentRawMessage.type === 'init' && agentRawMessage.session_id) {
           this.onSessionUpdate?.(agentRawMessage.session_id)
+        } else if (agentRawMessage.type === 'compact' && agentRawMessage.session_id) {
+          this.onSessionUpdate?.(agentRawMessage.session_id)
         }
         this.onChunk({
           type: ChunkType.RAW,
@@ -143,6 +150,7 @@ export class AiSdkToChunkAdapter {
         })
         break
       case 'text-delta': {
+        this.hasTextContent = true
         const processedText = chunk.text || ''
         let finalText: string
 
@@ -301,6 +309,25 @@ export class AiSdkToChunkAdapter {
       }
 
       case 'finish': {
+        // Check if session was cleared (e.g., /clear command) and no text was output
+        const sessionCleared = this.getSessionWasCleared?.() ?? false
+        if (sessionCleared && !this.hasTextContent) {
+          // Inject a "context cleared" message for the user
+          const clearMessage = '✨ Context cleared. Starting fresh conversation.'
+          this.onChunk({
+            type: ChunkType.TEXT_START
+          })
+          this.onChunk({
+            type: ChunkType.TEXT_DELTA,
+            text: clearMessage
+          })
+          this.onChunk({
+            type: ChunkType.TEXT_COMPLETE,
+            text: clearMessage
+          })
+          final.text = clearMessage
+        }
+
         const usage = {
           completion_tokens: chunk.totalUsage?.outputTokens || 0,
           prompt_tokens: chunk.totalUsage?.inputTokens || 0,
diff --git a/src/aiCore/index_new.ts b/src/aiCore/index_new.ts
index 800d2ff..434b232 100644
--- a/src/aiCore/index_new.ts
+++ b/src/aiCore/index_new.ts
@@ -7,16 +7,17 @@
  * 2. 暂时保持接口兼容性
  */
 
+import type { GatewayLanguageModelEntry } from '@ai-sdk/gateway'
 import { createExecutor } from '@cherrystudio/ai-core'
 import { loggerService } from '@logger'
 import { getEnableDeveloperMode } from '@renderer/hooks/useSettings'
 import { addSpan, endSpan } from '@renderer/services/SpanManagerService'
 import type { StartSpanParams } from '@renderer/trace/types/ModelSpanEntity'
-import type { Assistant, GenerateImageParams, Model, Provider } from '@renderer/types'
+import { type Assistant, type GenerateImageParams, type Model, type Provider, SystemProviderIds } from '@renderer/types'
 import type { AiSdkModel, StreamTextParams } from '@renderer/types/aiCoreTypes'
 import { SUPPORTED_IMAGE_ENDPOINT_LIST } from '@renderer/utils'
 import { buildClaudeCodeSystemModelMessage } from '@shared/anthropic'
-import { type ImageModel, type LanguageModel, type Provider as AiSdkProvider, wrapLanguageModel } from 'ai'
+import { gateway, type ImageModel, type LanguageModel, type Provider as AiSdkProvider, wrapLanguageModel } from 'ai'
 
 import AiSdkToChunkAdapter from './chunk/AiSdkToChunkAdapter'
 import LegacyAiProvider from './legacy/index'
@@ -439,6 +440,18 @@ export default class ModernAiProvider {
 
   // 代理其他方法到原有实现
   public async models() {
+    if (this.actualProvider.id === SystemProviderIds['ai-gateway']) {
+      const formatModel = function (models: GatewayLanguageModelEntry[]): Model[] {
+        return models.map((m) => ({
+          id: m.id,
+          name: m.name,
+          provider: 'gateway',
+          group: m.id.split('/')[0],
+          description: m.description ?? undefined
+        }))
+      }
+      return formatModel((await gateway.getAvailableModels()).models)
+    }
     return this.legacyProvider.models()
   }
 
diff --git a/src/aiCore/legacy/clients/openai/OpenAIResponseAPIClient.ts b/src/aiCore/legacy/clients/openai/OpenAIResponseAPIClient.ts
index b9131be..40cace5 100644
--- a/src/aiCore/legacy/clients/openai/OpenAIResponseAPIClient.ts
+++ b/src/aiCore/legacy/clients/openai/OpenAIResponseAPIClient.ts
@@ -90,7 +90,7 @@ export class OpenAIResponseAPIClient extends OpenAIBaseClient<
     if (isOpenAILLMModel(model) && !isOpenAIChatCompletionOnlyModel(model)) {
       if (this.provider.id === 'azure-openai' || this.provider.type === 'azure-openai') {
         this.provider = { ...this.provider, apiHost: this.formatApiHost() }
-        if (this.provider.apiVersion === 'preview') {
+        if (this.provider.apiVersion === 'preview' || this.provider.apiVersion === 'v1') {
           return this
         } else {
           return this.client
diff --git a/src/aiCore/provider/__tests__/providerConfig.test.ts b/src/aiCore/provider/__tests__/providerConfig.test.ts
index cc5f20c..3978623 100644
--- a/src/aiCore/provider/__tests__/providerConfig.test.ts
+++ b/src/aiCore/provider/__tests__/providerConfig.test.ts
@@ -39,6 +39,7 @@ vi.mock('@renderer/config/providers', async (importOriginal) => {
   return {
     ...actual,
     isCherryAIProvider: vi.fn(),
+    isPerplexityProvider: vi.fn(),
     isAnthropicProvider: vi.fn(() => false),
     isAzureOpenAIProvider: vi.fn(() => false),
     isGeminiProvider: vi.fn(() => false),
@@ -52,7 +53,7 @@ vi.mock('@renderer/hooks/useVertexAI', () => ({
   createVertexProvider: vi.fn()
 }))
 
-import { isCherryAIProvider } from '@renderer/config/providers'
+import { isCherryAIProvider, isPerplexityProvider } from '@renderer/config/providers'
 import { getProviderByModel } from '@renderer/services/AssistantService'
 import type { Model, Provider } from '@renderer/types'
 import { formatApiHost } from '@renderer/utils/api'
@@ -97,6 +98,16 @@ const createCherryAIProvider = (): Provider => ({
   isSystem: false
 })
 
+const createPerplexityProvider = (): Provider => ({
+  id: 'perplexity',
+  type: 'openai',
+  name: 'Perplexity',
+  apiKey: 'test-key',
+  apiHost: 'https://api.perplexity.ai',
+  models: [],
+  isSystem: false
+})
+
 describe('Copilot responses routing', () => {
   beforeEach(() => {
     ;(globalThis as any).window = {
@@ -195,3 +206,70 @@ describe('CherryAI provider configuration', () => {
     expect(actualProvider.apiHost).toBe('')
   })
 })
+
+describe('Perplexity provider configuration', () => {
+  beforeEach(() => {
+    ;(globalThis as any).window = {
+      ...(globalThis as any).window,
+      keyv: createWindowKeyv()
+    }
+    vi.clearAllMocks()
+  })
+
+  it('formats Perplexity provider apiHost with false parameter', () => {
+    const provider = createPerplexityProvider()
+    const model = createModel('sonar', 'Sonar', 'perplexity')
+
+    // Mock the functions to simulate Perplexity provider detection
+    vi.mocked(isCherryAIProvider).mockReturnValue(false)
+    vi.mocked(isPerplexityProvider).mockReturnValue(true)
+    vi.mocked(getProviderByModel).mockReturnValue(provider)
+
+    // Call getActualProvider which should trigger formatProviderApiHost
+    const actualProvider = getActualProvider(model)
+
+    // Verify that formatApiHost was called with false as the second parameter
+    expect(formatApiHost).toHaveBeenCalledWith('https://api.perplexity.ai', false)
+    expect(actualProvider.apiHost).toBe('https://api.perplexity.ai')
+  })
+
+  it('does not format non-Perplexity provider with false parameter', () => {
+    const provider = {
+      id: 'openai',
+      type: 'openai',
+      name: 'OpenAI',
+      apiKey: 'test-key',
+      apiHost: 'https://api.openai.com',
+      models: [],
+      isSystem: false
+    } as Provider
+    const model = createModel('gpt-4', 'GPT-4', 'openai')
+
+    // Mock the functions to simulate non-Perplexity provider
+    vi.mocked(isCherryAIProvider).mockReturnValue(false)
+    vi.mocked(isPerplexityProvider).mockReturnValue(false)
+    vi.mocked(getProviderByModel).mockReturnValue(provider)
+
+    // Call getActualProvider
+    const actualProvider = getActualProvider(model)
+
+    // Verify that formatApiHost was called with default parameters (true)
+    expect(formatApiHost).toHaveBeenCalledWith('https://api.openai.com')
+    expect(actualProvider.apiHost).toBe('https://api.openai.com/v1')
+  })
+
+  it('handles Perplexity provider with empty apiHost', () => {
+    const provider = createPerplexityProvider()
+    provider.apiHost = ''
+    const model = createModel('sonar', 'Sonar', 'perplexity')
+
+    vi.mocked(isCherryAIProvider).mockReturnValue(false)
+    vi.mocked(isPerplexityProvider).mockReturnValue(true)
+    vi.mocked(getProviderByModel).mockReturnValue(provider)
+
+    const actualProvider = getActualProvider(model)
+
+    expect(formatApiHost).toHaveBeenCalledWith('', false)
+    expect(actualProvider.apiHost).toBe('')
+  })
+})
diff --git a/src/aiCore/provider/factory.ts b/src/aiCore/provider/factory.ts
index 4cdbfb6..569b562 100644
--- a/src/aiCore/provider/factory.ts
+++ b/src/aiCore/provider/factory.ts
@@ -84,6 +84,8 @@ export async function createAiSdkProvider(config) {
       config.providerId = `${config.providerId}-chat`
     } else if (config.providerId === 'azure' && config.options?.mode === 'responses') {
       config.providerId = `${config.providerId}-responses`
+    } else if (config.providerId === 'cherryin' && config.options?.mode === 'chat') {
+      config.providerId = 'cherryin-chat'
     }
     localProvider = await createProviderCore(config.providerId, config.options)
 
diff --git a/src/aiCore/provider/providerConfig.ts b/src/aiCore/provider/providerConfig.ts
index 4669d4c..07b4cea 100644
--- a/src/aiCore/provider/providerConfig.ts
+++ b/src/aiCore/provider/providerConfig.ts
@@ -11,7 +11,8 @@ import {
   isAzureOpenAIProvider,
   isCherryAIProvider,
   isGeminiProvider,
-  isNewApiProvider
+  isNewApiProvider,
+  isPerplexityProvider
 } from '@renderer/config/providers'
 import {
   getAwsBedrockAccessKeyId,
@@ -103,6 +104,8 @@ function formatProviderApiHost(provider: Provider): Provider {
     formatted.apiHost = formatVertexApiHost(formatted)
   } else if (isCherryAIProvider(formatted)) {
     formatted.apiHost = formatApiHost(formatted.apiHost, false)
+  } else if (isPerplexityProvider(formatted)) {
+    formatted.apiHost = formatApiHost(formatted.apiHost, false)
   } else {
     formatted.apiHost = formatApiHost(formatted.apiHost)
   }
@@ -168,7 +171,7 @@ export function providerToAiSdkConfig(
   extraOptions.endpoint = endpoint
   if (actualProvider.type === 'openai-response' && !isOpenAIChatCompletionOnlyModel(model)) {
     extraOptions.mode = 'responses'
-  } else if (aiSdkProviderId === 'openai') {
+  } else if (aiSdkProviderId === 'openai' || (aiSdkProviderId === 'cherryin' && actualProvider.type === 'openai')) {
     extraOptions.mode = 'chat'
   }
 
@@ -186,9 +189,11 @@ export function providerToAiSdkConfig(
     }
   }
   // azure
+  // https://learn.microsoft.com/en-us/azure/ai-foundry/openai/latest
+  // https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/responses?tabs=python-key#responses-api
   if (aiSdkProviderId === 'azure' || actualProvider.type === 'azure-openai') {
-    // extraOptions.apiVersion = actualProvider.apiVersion 默认使用v1，不使用azure endpoint
-    if (actualProvider.apiVersion === 'preview') {
+    // extraOptions.apiVersion = actualProvider.apiVersion === 'preview' ? 'v1' : actualProvider.apiVersion 默认使用v1，不使用azure endpoint
+    if (actualProvider.apiVersion === 'preview' || actualProvider.apiVersion === 'v1') {
       extraOptions.mode = 'responses'
     } else {
       extraOptions.mode = 'chat'
diff --git a/src/aiCore/provider/providerInitialization.ts b/src/aiCore/provider/providerInitialization.ts
index 665f2bd..baf4005 100644
--- a/src/aiCore/provider/providerInitialization.ts
+++ b/src/aiCore/provider/providerInitialization.ts
@@ -71,6 +71,21 @@ export const NEW_PROVIDER_CONFIGS: ProviderConfig[] = [
     creatorFunctionName: 'createHuggingFace',
     supportsImageGeneration: true,
     aliases: ['hf', 'hugging-face']
+  },
+  {
+    id: 'ai-gateway',
+    name: 'AI Gateway',
+    import: () => import('@ai-sdk/gateway'),
+    creatorFunctionName: 'createGateway',
+    supportsImageGeneration: true,
+    aliases: ['gateway']
+  },
+  {
+    id: 'cerebras',
+    name: 'Cerebras',
+    import: () => import('@ai-sdk/cerebras'),
+    creatorFunctionName: 'createCerebras',
+    supportsImageGeneration: false
   }
 ] as const
 
diff --git a/src/aiCore/utils/options.ts b/src/aiCore/utils/options.ts
index 60d9b1e..88f5564 100644
--- a/src/aiCore/utils/options.ts
+++ b/src/aiCore/utils/options.ts
@@ -113,6 +113,9 @@ export function buildProviderOptions(
         }
         break
       }
+      case 'cherryin':
+        providerSpecificOptions = buildCherryInProviderOptions(assistant, model, capabilities, actualProvider)
+        break
       default:
         throw new Error(`Unsupported base provider ${baseProviderId}`)
     }
@@ -148,11 +151,12 @@ export function buildProviderOptions(
     ...providerSpecificOptions,
     ...getCustomParameters(assistant)
   }
-  // vertex需要映射到google或anthropic
+
   const rawProviderKey =
     {
       'google-vertex': 'google',
-      'google-vertex-anthropic': 'anthropic'
+      'google-vertex-anthropic': 'anthropic',
+      'ai-gateway': 'gateway'
     }[rawProviderId] || rawProviderId
 
   // 返回 AI Core SDK 要求的格式：{ 'providerId': providerOptions }
@@ -270,6 +274,34 @@ function buildXAIProviderOptions(
   return providerOptions
 }
 
+function buildCherryInProviderOptions(
+  assistant: Assistant,
+  model: Model,
+  capabilities: {
+    enableReasoning: boolean
+    enableWebSearch: boolean
+    enableGenerateImage: boolean
+  },
+  actualProvider: Provider
+): Record<string, any> {
+  const serviceTierSetting = getServiceTier(model, actualProvider)
+
+  switch (actualProvider.type) {
+    case 'openai':
+      return {
+        ...buildOpenAIProviderOptions(assistant, model, capabilities),
+        serviceTier: serviceTierSetting
+      }
+
+    case 'anthropic':
+      return buildAnthropicProviderOptions(assistant, model, capabilities)
+
+    case 'gemini':
+      return buildGeminiProviderOptions(assistant, model, capabilities)
+  }
+  return {}
+}
+
 /**
  * Build Bedrock providerOptions
  */
diff --git a/src/aiCore/utils/reasoning.ts b/src/aiCore/utils/reasoning.ts
index 3a36fb6..d0b6f1d 100644
--- a/src/aiCore/utils/reasoning.ts
+++ b/src/aiCore/utils/reasoning.ts
@@ -109,6 +109,11 @@ export function getReasoningEffort(assistant: Assistant, model: Model): Reasonin
 
     // use thinking, doubao, zhipu, etc.
     if (isSupportedThinkingTokenDoubaoModel(model) || isSupportedThinkingTokenZhipuModel(model)) {
+      if (provider.id === SystemProviderIds.cerebras) {
+        return {
+          disable_reasoning: true
+        }
+      }
       return { thinking: { type: 'disabled' } }
     }
 
@@ -306,6 +311,9 @@ export function getReasoningEffort(assistant: Assistant, model: Model): Reasonin
     return {}
   }
   if (isSupportedThinkingTokenZhipuModel(model)) {
+    if (provider.id === SystemProviderIds.cerebras) {
+      return {}
+    }
     return { thinking: { type: 'enabled' } }
   }
 
@@ -418,6 +426,8 @@ export function getAnthropicReasoningParams(assistant: Assistant, model: Model):
 /**
  * 获取 Gemini 推理参数
  * 从 GeminiAPIClient 中提取的逻辑
+ * 注意：Gemini/GCP 端点所使用的 thinkingBudget 等参数应该按照驼峰命名法传递
+ * 而在 Google 官方提供的 OpenAI 兼容端点中则使用蛇形命名法 thinking_budget
  */
 export function getGeminiReasoningParams(assistant: Assistant, model: Model): Record<string, any> {
   if (!isReasoningModel(model)) {
@@ -431,8 +441,8 @@ export function getGeminiReasoningParams(assistant: Assistant, model: Model): Re
     if (reasoningEffort === undefined) {
       return {
         thinkingConfig: {
-          include_thoughts: false,
-          ...(GEMINI_FLASH_MODEL_REGEX.test(model.id) ? { thinking_budget: 0 } : {})
+          includeThoughts: false,
+          ...(GEMINI_FLASH_MODEL_REGEX.test(model.id) ? { thinkingBudget: 0 } : {})
         }
       }
     }
@@ -442,7 +452,7 @@ export function getGeminiReasoningParams(assistant: Assistant, model: Model): Re
     if (effortRatio > 1) {
       return {
         thinkingConfig: {
-          include_thoughts: true
+          includeThoughts: true
         }
       }
     }
@@ -452,8 +462,8 @@ export function getGeminiReasoningParams(assistant: Assistant, model: Model): Re
 
     return {
       thinkingConfig: {
-        ...(budget > 0 ? { thinking_budget: budget } : {}),
-        include_thoughts: true
+        ...(budget > 0 ? { thinkingBudget: budget } : {}),
+        includeThoughts: true
       }
     }
   }
diff --git a/src/aiCore/utils/websearch.ts b/src/aiCore/utils/websearch.ts
index fde4ff5..02619b5 100644
--- a/src/aiCore/utils/websearch.ts
+++ b/src/aiCore/utils/websearch.ts
@@ -107,6 +107,11 @@ export function buildProviderBuiltinWebSearchConfig(
         }
       }
     }
+    case 'cherryin': {
+      const _providerId =
+        { 'openai-response': 'openai', openai: 'openai-chat' }[model?.endpoint_type ?? ''] ?? model?.endpoint_type
+      return buildProviderBuiltinWebSearchConfig(_providerId, webSearchConfig, model)
+    }
     default: {
       return {}
     }
